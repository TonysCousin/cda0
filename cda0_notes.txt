Notes on the cda0 project
=========================

10/3/22:
* Spent approx 3 hr setting up & beginning the coding.

10/4/22:
* Time spent:  
  17:00 - 18:44 = 1:44
  21:21 - 22:34 = 1:13
* Decided to go forward with the somewhat awkward and non-extensible approach of modeling observations to include states on
  exactly 3 neighbor vehicles.  For a future version I will replace that with a more general approach that looks at the
  roadway itself (analogous to an Atari agent viewing screen pixels rather than tracking a number of alien ships).

10/5/22:
* Time 20:45 - 22:40 = 1:55

10/6/22:
* Time
  10:33 - 12:39 = 2:06
  21:53 - 23:03 = 1:10

10/7/22:
* Time
  21:24 - 22:42 = 1:18
  23:01 - 00:30 = 1:29

10/8/22:
* Time
  17:04 - 18:25 = 1:21
  19:03 - 19:27 = 0:24

10/9/22:
* Time 19:54 - 22:53, int 13 = 2:46
* Completed unit testing of all env logic except reward.
* Built initial reward logic.

10/10/22:
* time 10:48 - 11:15 = 0:27

10/11/22:
* Time
  21:08 - 23:15 = 2:07
  23:44 - 00:15 = 0:31

10/12/22:
* Time
  08:14 - 09:40 = 1:26
  14:12 - 14:57 = 0:45
  15:27 - 15:56 = 0:29
* Fixed all unit test and run-time errors. Ready to begin training.

10/13/22:
* Time 22:08 - 01:00 = 2:52
* Still having occasional problems with observation_space violations

10/14/22:
* Time 23:08 - 01:59 = 2:51
* Created conda env "cda0" as a copy of rllib2 to make sure any project-specific changes aren't applied to the base env.

10/15/22:
* Time
  11:59 - 12:34 = 0:35
  22:44 - 00:20 = 1:36
* SUCCESS - first taste! got several trials to drive lane 0 beginning to end, forcing no steering.
  Best tuning trial was 35915_00003; saving checkpoint 181.
* Ran new training job (7559b), with results under the cda0-l0-free dir, that starts agent in lane 0 but allows it to change
  lanes.  Most successful trial was 75599b_00005; using checkpoint 126.  PPO lr = 2.8e-5, batch = 512.
* TODO: Need an inference program to run these successful models and capture their trajectories for viewing.

10/16/22:
* Time 21:05 - 23:09, int 20 = 1:44
* New tuning job (17669) under cda0-l01-free dir. This one randomly initializes episodes with the ego vehicle in either lane
  0 or 1, but not 2.  The neighbor vehicles still do not move. Two solutions got pretty close (00003 and 00002), but none
  scored higher than low 1.8s for the mean reward.
* Installed pygame (with pip) into the conda env "cda0" to experiment with making a graphical display of the simulation.
  Played with an example program enough to quickly understand how to do some basic graphics needed for my sim.

10/17/22:
* Time
  14:16 - 14:48 = 0:32
  17:10 - 18:36 = 1:26
  22:32 - 23:21 = 0:49
* Fixed a problem in the env wrapper that was locking the ego lane to 0; also changed reward shape some, so all previous
  training needs to be thrown away.
* Training again on random lane start (0 or 1 only) with no neighbor vehicles moving; trial ID = c2d96.
	* Trial 0003 ran for a long time, but reward gradually increased the whole time, maxing out above 4!  Not sure how
	  this is possible. LR = 0.00013, batch = 512.  It has learned to shy away from speeds near the max, since it gets
	  punished for large accels there; also shy of speeds near zero, for same reason, but this is less of a fear.
	  Result is that accel oscillates between +3 and -3 m/s^2, with speeds going between 4% and 75% of max range.
	  So avg speed of the vehicle is really small, and it collects more reward points for staying on track.
	* Trial 0007 was the only other one that succeeded, with a max reward around 1.9. LR = 0.000161, batch = 256.
* Increased penalty for jerk; decreased existential time step reward; made a more differentiable shape for penalty where
  accel is threatening to take vehicle past either high or low speed limit, in order to minimize accel oscillations.
* Training again on random lane start (0 or 1); trial ID = 47fd7. None of the 18 trials converged.

10/18/22:
* Time 21:40 - 11:00 = 1:20
* Removed reward penalties for both the acceleration limits and trying to keep steering command near {-1, 0, 1}.
  Ran tune (ID 10baa). Died in middle due to computer crash.
* Compared code to that used on 10/16 (commit cf6f) to find why suddenly nothing is learning to drive straight.
  Only found two seemingly minor diffs in the reward structure (given that the new penalties are commented out).
  I changed those back to the way they were on 10/16 and ran a new tuning run (b24d4).

----- time logged to Process Dashboard to here -----

10/19/22:
* Time
  17:46 - 18:25 = 0:39
  21:54 - 23:03 = 1:10
* Last night's run produced 3 winners (out of 20 trials). This is with the "old" reward structure, so just a baseline to 
  prove it can be trained.
	* Trial 14 had mean reward = 1.87; LR = 5.54e-5, activation = relu, network = [300, 128, 64], batch = 2048. Solved
	  in 121 iters. Its min reward was slightly < 0, however, so not idea.
	* Trial 16 had mean reward = 1.88 with similar min; LR = 3.42e-5, activation = relu, network = [300, 128, 64], batch = 512.
	* Trial 19 had mean reward = 1.89 with similar min; LR = 2.11e-5, activation = relu, network = [256, 256], batch = 512.
	  Use checkpoint 163.
	* Reviewing these results, I realized the reward was broken for lane change penalty, so fixed it.
* Another run (f7b05) applied these changes.  Found several successful trials.
	* Trial 19 had mean reward = 2.02 with similar min; LR = 1.36e-5, activation = tanh, network = [300, 128, 64], batch = 256.
	  Use checkpoint 154. 
	* Trial 18 had mean reward = 1.89 with min ~1.3; LR = 2.44e-5, activation = tanh, network = [300, 128, 64], batch = 1024.
	* Trial 15 had mean reward = 1.83 with an unimpressive min; LR = 5.60e-5, activation = relu, network = [256, 256], batch = 512.
	  Use checkpoint 126.
	* Trial 4 had mean reward = 1.86 with min ~1.3; LR = 3.39e-5, activation = relu, network = [300, 128, 64], batch = 2048.
	  Use checkpoint 97.

10/20/22:
* Time
  01:12 - 01:33 = 0:21
  17:15 - 18:31 = 1:16
  19:54 - 
* Added the LCC penalty back into the reward method.  Made tuning run 211ad, all with a 3-layer network.
	* Trial 6 mean reward = 2.01, LR = 1.04e-4, activation = tanh, batch = 1024
	* Trial 11 mean reward = 1.75, LR = 1.99e-4, activation = tanh, batch = 1024
	* Trial 14 mean reward = 1.75, LR = 1.37e-5, activation = tanh, batch = 128; min reward above +1.5
	* Trial 18 mean reward = 1.76, LR = 1.67e-5, activation = tanh, batch = 128, min reward above +1.6
* Run 211ad showed
	* LC cmd penalty worked really well at keeping the LC command very close to zero
	* Accel was also really close to zero on trial 6, but averaged slightly positive so that speed rose throughout, but it never
	  reached the speed limit.  Trial 6 got only 0.796 completion reward, but gathered lots of time step rewards throughout.
	* Trial 11 had big accel, but its total reward was ~1.8 vs 2.2 for trial 6. So avg time step reward of ~0.09 is too high.
	* There is a strong desire to drive in lane 0, whether the initial lane is 0 or 1; LC occurs immediately if needed.
	* Max completion reward is only ~0.83 for the fastest possible travel time.  Needs to be increased.
* Added scaling of the action_space in the wrapper class to keep the NN output accel limited to [-1, 1] (it was in [-3, 3]).
  Modified the reward shape a bit to better emphasize completing the course as fast as possible.
* New tuning run (b5410) to finalize work on lane change issues.

10/21/22:
* Time 16:52 - , int 23
* One run from yesterday (b4510) succeeded, which was trial 1. Mean reward = 2.11, LR = 4.99e-5, activation = tanh, batch = 1024.
  Actions were well behaved, as desired, but accels were all small and tended to slow the car down to get more time step rewards.
	* Changed reward limits from [-1, 1] to [-2, 2], since the completion reward was being greatly clipped.
	* Reduced jerk penalty mult from 0.01 to 0.005.
	* Reduced time step reward from 0.005 to 0.002 (it was contributing >0.5 to the total episode reward)
	* Added penalty for exceeding speed limit (and increased obs space upper limit for speed substantially to allow an excess).
