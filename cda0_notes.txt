Notes on the cda0 project
=========================

10/3/22:
* Spent approx 3 hr setting up & beginning the coding.

10/4/22:
* Time spent:  
  17:00 - 18:44 = 1:44
  21:21 - 22:34 = 1:13
* Decided to go forward with the somewhat awkward and non-extensible approach of modeling observations to include states on
  exactly 3 neighbor vehicles.  For a future version I will replace that with a more general approach that looks at the
  roadway itself (analogous to an Atari agent viewing screen pixels rather than tracking a number of alien ships).

10/5/22:
* Time 20:45 - 22:40 = 1:55

10/6/22:
* Time
  10:33 - 12:39 = 2:06
  21:53 - 23:03 = 1:10

10/7/22:
* Time
  21:24 - 22:42 = 1:18
  23:01 - 00:30 = 1:29

10/8/22:
* Time
  17:04 - 18:25 = 1:21
  19:03 - 19:27 = 0:24

10/9/22:
* Time 19:54 - 22:53, int 13 = 2:46
* Completed unit testing of all env logic except reward.
* Built initial reward logic.

10/10/22:
* time 10:48 - 11:15 = 0:27

10/11/22:
* Time
  21:08 - 23:15 = 2:07
  23:44 - 00:15 = 0:31

10/12/22:
* Time
  08:14 - 09:40 = 1:26
  14:12 - 14:57 = 0:45
  15:27 - 15:56 = 0:29
* Fixed all unit test and run-time errors. Ready to begin training.

10/13/22:
* Time 22:08 - 01:00 = 2:52
* Still having occasional problems with observation_space violations

10/14/22:
* Time 23:08 - 01:59 = 2:51
* Created conda env "cda0" as a copy of rllib2 to make sure any project-specific changes aren't applied to the base env.

10/15/22:
* Time
  11:59 - 12:34 = 0:35
  22:44 - 00:20 = 1:36
* SUCCESS - first taste! got several trials to drive lane 0 beginning to end, forcing no steering.
  Best tuning trial was 35915_00003; saving checkpoint 181.
* Ran new training job (7559b), with results under the cda0-l0-free dir, that starts agent in lane 0 but allows it to change
  lanes.  Most successful trial was 75599b_00005; using checkpoint 126.  PPO lr = 2.8e-5, batch = 512.
* TODO: Need an inference program to run these successful models and capture their trajectories for viewing.

10/16/22:
* Time 21:05 - 23:09, int 20 = 1:44
* New tuning job (17669) under cda0-l01-free dir. This one randomly initializes episodes with the ego vehicle in either lane
  0 or 1, but not 2.  The neighbor vehicles still do not move. Two solutions got pretty close (00003 and 00002), but none
  scored higher than low 1.8s for the mean reward.
* Installed pygame (with pip) into the conda env "cda0" to experiment with making a graphical display of the simulation.
  Played with an example program enough to quickly understand how to do some basic graphics needed for my sim.

10/17/22:
* Time
  14:16 - 14:48 = 0:32
  17:10 - 18:36 = 1:26
  22:32 - 23:21 = 0:49
* Fixed a problem in the env wrapper that was locking the ego lane to 0; also changed reward shape some, so all previous
  training needs to be thrown away.
* Training again on random lane start (0 or 1 only) with no neighbor vehicles moving; trial ID = c2d96.
	* Trial 0003 ran for a long time, but reward gradually increased the whole time, maxing out above 4!  Not sure how
	  this is possible. LR = 0.00013, batch = 512.  It has learned to shy away from speeds near the max, since it gets
	  punished for large accels there; also shy of speeds near zero, for same reason, but this is less of a fear.
	  Result is that accel oscillates between +3 and -3 m/s^2, with speeds going between 4% and 75% of max range.
	  So avg speed of the vehicle is really small, and it collects more reward points for staying on track.
	* Trial 0007 was the only other one that succeeded, with a max reward around 1.9. LR = 0.000161, batch = 256.
* Increased penalty for jerk; decreased existential time step reward; made a more differentiable shape for penalty where
  accel is threatening to take vehicle past either high or low speed limit, in order to minimize accel oscillations.
* Training again on random lane start (0 or 1); trial ID = 47fd7. None of the 18 trials converged.

10/18/22:
* Time 21:40 - 11:00 = 1:20
* Removed reward penalties for both the acceleration limits and trying to keep steering command near {-1, 0, 1}.
  Ran tune (ID 10baa). Died in middle due to computer crash.
* Compared code to that used on 10/16 (commit cf6f) to find why suddenly nothing is learning to drive straight.
  Only found two seemingly minor diffs in the reward structure (given that the new penalties are commented out).
  I changed those back to the way they were on 10/16 and ran a new tuning run (b24d4).

10/19/22:
* Time
  17:46 - 18:25 = 0:39
  21:54 - 23:03 = 1:10
* Last night's run produced 3 winners (out of 20 trials). This is with the "old" reward structure, so just a baseline to 
  prove it can be trained.
	* Trial 14 had mean reward = 1.87; LR = 5.54e-5, activation = relu, network = [300, 128, 64], batch = 2048. Solved
	  in 121 iters. Its min reward was slightly < 0, however, so not idea.
	* Trial 16 had mean reward = 1.88 with similar min; LR = 3.42e-5, activation = relu, network = [300, 128, 64], batch = 512.
	* Trial 19 had mean reward = 1.89 with similar min; LR = 2.11e-5, activation = relu, network = [256, 256], batch = 512.
	  Use checkpoint 163.
	* Reviewing these results, I realized the reward was broken for lane change penalty, so fixed it.
* Another run (f7b05) applied these changes.  Found several successful trials.
	* Trial 19 had mean reward = 2.02 with similar min; LR = 1.36e-5, activation = tanh, network = [300, 128, 64], batch = 256.
	  Use checkpoint 154. 
	* Trial 18 had mean reward = 1.89 with min ~1.3; LR = 2.44e-5, activation = tanh, network = [300, 128, 64], batch = 1024.
	* Trial 15 had mean reward = 1.83 with an unimpressive min; LR = 5.60e-5, activation = relu, network = [256, 256], batch = 512.
	  Use checkpoint 126.
	* Trial 4 had mean reward = 1.86 with min ~1.3; LR = 3.39e-5, activation = relu, network = [300, 128, 64], batch = 2048.
	  Use checkpoint 97.

10/20/22:
* Time
  01:12 - 01:33 = 0:21
  17:15 - 18:31 = 1:16
  19:54 - 10:00 = 2:06
* Added the LCC penalty back into the reward method.  Made tuning run 211ad, all with a 3-layer network.
	* Trial 6 mean reward = 2.01, LR = 1.04e-4, activation = tanh, batch = 1024
	* Trial 11 mean reward = 1.75, LR = 1.99e-4, activation = tanh, batch = 1024
	* Trial 14 mean reward = 1.75, LR = 1.37e-5, activation = tanh, batch = 128; min reward above +1.5
	* Trial 18 mean reward = 1.76, LR = 1.67e-5, activation = tanh, batch = 128, min reward above +1.6
* Run 211ad showed
	* LC cmd penalty worked really well at keeping the LC command very close to zero
	* Accel was also really close to zero on trial 6, but averaged slightly positive so that speed rose throughout, but it never
	  reached the speed limit.  Trial 6 got only 0.796 completion reward, but gathered lots of time step rewards throughout.
	* Trial 11 had big accel, but its total reward was ~1.8 vs 2.2 for trial 6. So avg time step reward of ~0.09 is too high.
	* There is a strong desire to drive in lane 0, whether the initial lane is 0 or 1; LC occurs immediately if needed.
	* Max completion reward is only ~0.83 for the fastest possible travel time.  Needs to be increased.
* Added scaling of the action_space in the wrapper class to keep the NN output accel limited to [-1, 1] (it was in [-3, 3]).
  Modified the reward shape a bit to better emphasize completing the course as fast as possible.
* New tuning run (b5410) to finalize work on lane change issues.

10/21/22:
* Time
  16:52 - 19:37, int 23 = 2:22
  20:12 - 21:12 = 1:00
* One run from yesterday (b4510) succeeded, which was trial 1. Mean reward = 2.11, LR = 4.99e-5, activation = tanh, batch = 1024.
  Actions were well behaved, as desired, but accels were all small and tended to slow the car down to get more time step rewards.
	* Changed reward limits from [-1, 1] to [-2, 2], since the completion reward was being greatly clipped.
	* Reduced jerk penalty mult from 0.01 to 0.005.
	* Reduced time step reward from 0.005 to 0.002 (it was contributing >0.5 to the total episode reward)
	* Added penalty for exceeding speed limit (and increased obs space upper limit for speed substantially to allow an excess).
* Run fe601 with these changes produced no successful trials.  After observing one slightly promising run, made these changes:
	* Increased gamma from 0.99 to 0.999.
	* Reduced LR range a bit.
	* Added a HP to choose model's post_fcnet_activation between relu & tanh (was formerly fixed at relu).
* Another run with the above changes had no success either.  So I removed the speed limit penalty and created run 25ee5.
* During training, I continued to write the graphics code, but in separate copies of the files: inference.py,
  simple_highway_with_ramp.py, using the suffix "_new" on each one, so it won't affect the ongoing training.

10/22/22:
* Time
  10:18 - 11:00 = 0:42
  11:34 - 12:55 = 1:21
  13:50 - 15:01 = 1:11
  17:40 - 18:03 = 0:23
  19:55 - 21:25 = 1:30
* Runs from last night (92d01) that look promising are 3 (LR 9.36e-5, tanh/tanh, batch 1024) and 12 (LR 3.50e-5, relu/tanh, batch 512).
	* Run 3 ran close to 0 speed. When it finished, it collected 0 completion reward because it took 1176 time steps!
* Added penalty for slow speeds (normalized speed < 0.5), slightly increased penalty for jerk and slightly reduced penalty for 
  lane change command.
* Increased number of most_recent iterations to evaluate for stopping, and didn't stop if max reward is close to success threshold.
* Finished the graphics code for the initial roadway display and integrated it.
* Realized a MAJOR PROBLEM I have had: the training was always starting in lane 0. Also, since the vehicle initial conditions set by
  reset() were pretty limited (speed and location), it was often never seeing experiences for downtrack locations or for high speeds.
  Therefore, I modified reset() to randomize these, and the initial lane ID, over the full range of possible experiences.  It
  is not clear to me how to get Ray Tune to pass in random values for each episode (when reset() is called), so for now I'll
  depend on reset() to handle it.  I've added a "training" config option that opens up these random ranges; if it is False or
  undefined, the initial conditions will be as they were before.

----- time logged to Process Dashboard to here -----

10/23/22:
* Time
  15:11 - 15:30 = 0:19
  20:16 - 21:03 = 0:47
* Finally got one that seems to have learned: 10965 trial 10, used LR 5.80e-5, network [200, 100, 20] and output activation = tanh.
  However, this also did not perform well.  Accelerations are all over the place, and LC commands are as well.  Several inference
  runs failed to complete even half of the track.
* Added remaining code to the Graphics class in the inference program to do rudimentary display of ego vehicle driving on the track.
* Started a new training run using the DDPG algo instead of PPO (which I had been using thus far).  It seemed to produce some
  good results quickly, but the rewards never grew enough.  Started playing with the network structures more.
	* Some of the iterations are showing min rewards in the -200 to -300 range - how is this possible?
