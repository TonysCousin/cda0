Notes on the cda0 project
=========================

10/3/22:
* Spent approx 3 hr setting up & beginning the coding.

10/4/22:
* Time spent:  
  17:00 - 18:44 = 1:44
  21:21 - 22:34 = 1:13
* Decided to go forward with the somewhat awkward and non-extensible approach of modeling observations to include states on
  exactly 3 neighbor vehicles.  For a future version I will replace that with a more general approach that looks at the
  roadway itself (analogous to an Atari agent viewing screen pixels rather than tracking a number of alien ships).

10/5/22:
* Time 20:45 - 22:40 = 1:55

10/6/22:
* Time
  10:33 - 12:39 = 2:06
  21:53 - 23:03 = 1:10

10/7/22:
* Time
  21:24 - 22:42 = 1:18
  23:01 - 00:30 = 1:29

10/8/22:
* Time
  17:04 - 18:25 = 1:21
  19:03 - 19:27 = 0:24

10/9/22:
* Time 19:54 - 22:53, int 13 = 2:46
* Completed unit testing of all env logic except reward.
* Built initial reward logic.

10/10/22:
* time 10:48 - 11:15 = 0:27

10/11/22:
* Time
  21:08 - 23:15 = 2:07
  23:44 - 00:15 = 0:31

10/12/22:
* Time
  08:14 - 09:40 = 1:26
  14:12 - 14:57 = 0:45
  15:27 - 15:56 = 0:29
* Fixed all unit test and run-time errors. Ready to begin training.

10/13/22:
* Time 22:08 - 01:00 = 2:52
* Still having occasional problems with observation_space violations

10/14/22:
* Time 23:08 - 01:59 = 2:51
* Created conda env "cda0" as a copy of rllib2 to make sure any project-specific changes aren't applied to the base env.

10/15/22:
* Time
  11:59 - 12:34 = 0:35
  22:44 - 00:20 = 1:36
* SUCCESS - first taste! got several trials to drive lane 0 beginning to end, forcing no steering.
  Best tuning trial was 35915_00003; saving checkpoint 181.
* Ran new training job (7559b), with results under the cda0-l0-free dir, that starts agent in lane 0 but allows it to change
  lanes.  Most successful trial was 75599b_00005; using checkpoint 126.  PPO lr = 2.8e-5, batch = 512.
* TODO: Need an inference program to run these successful models and capture their trajectories for viewing.

10/16/22:
* Time 21:05 - 23:09, int 20 = 1:44
* New tuning job (17669) under cda0-l01-free dir. This one randomly initializes episodes with the ego vehicle in either lane
  0 or 1, but not 2.  The neighbor vehicles still do not move. Two solutions got pretty close (00003 and 00002), but none
  scored higher than low 1.8s for the mean reward.
* Installed pygame (with pip) into the conda env "cda0" to experiment with making a graphical display of the simulation.
  Played with an example program enough to quickly understand how to do some basic graphics needed for my sim.

10/17/22:
* Time
  14:16 - 14:48 = 0:32
  17:10 - 18:36 = 1:26
  22:32 - 23:21 = 0:49
* Fixed a problem in the env wrapper that was locking the ego lane to 0; also changed reward shape some, so all previous
  training needs to be thrown away.
* Training again on random lane start (0 or 1 only) with no neighbor vehicles moving; trial ID = c2d96.
	* Trial 0003 ran for a long time, but reward gradually increased the whole time, maxing out above 4!  Not sure how
	  this is possible. LR = 0.00013, batch = 512.  It has learned to shy away from speeds near the max, since it gets
	  punished for large accels there; also shy of speeds near zero, for same reason, but this is less of a fear.
	  Result is that accel oscillates between +3 and -3 m/s^2, with speeds going between 4% and 75% of max range.
	  So avg speed of the vehicle is really small, and it collects more reward points for staying on track.
	* Trial 0007 was the only other one that succeeded, with a max reward around 1.9. LR = 0.000161, batch = 256.
* Increased penalty for jerk; decreased existential time step reward; made a more differentiable shape for penalty where
  accel is threatening to take vehicle past either high or low speed limit, in order to minimize accel oscillations.
* Training again on random lane start (0 or 1); trial ID = 47fd7. None of the 18 trials converged.

10/18/22:
* Time 21:40 - 11:00 = 1:20
* Removed reward penalties for both the acceleration limits and trying to keep steering command near {-1, 0, 1}.
  Ran tune (ID 10baa). Died in middle due to computer crash.
* Compared code to that used on 10/16 (commit cf6f) to find why suddenly nothing is learning to drive straight.
  Only found two seemingly minor diffs in the reward structure (given that the new penalties are commented out).
  I changed those back to the way they were on 10/16 and ran a new tuning run (b24d4).

10/19/22:
* Time
  17:46 - 18:25 = 0:39
  21:54 - 23:03 = 1:10
* Last night's run produced 3 winners (out of 20 trials). This is with the "old" reward structure, so just a baseline to 
  prove it can be trained.
	* Trial 14 had mean reward = 1.87; LR = 5.54e-5, activation = relu, network = [300, 128, 64], batch = 2048. Solved
	  in 121 iters. Its min reward was slightly < 0, however, so not idea.
	* Trial 16 had mean reward = 1.88 with similar min; LR = 3.42e-5, activation = relu, network = [300, 128, 64], batch = 512.
	* Trial 19 had mean reward = 1.89 with similar min; LR = 2.11e-5, activation = relu, network = [256, 256], batch = 512.
	  Use checkpoint 163.
	* Reviewing these results, I realized the reward was broken for lane change penalty, so fixed it.
* Another run (f7b05) applied these changes.  Found several successful trials.
	* Trial 19 had mean reward = 2.02 with similar min; LR = 1.36e-5, activation = tanh, network = [300, 128, 64], batch = 256.
	  Use checkpoint 154. 
	* Trial 18 had mean reward = 1.89 with min ~1.3; LR = 2.44e-5, activation = tanh, network = [300, 128, 64], batch = 1024.
	* Trial 15 had mean reward = 1.83 with an unimpressive min; LR = 5.60e-5, activation = relu, network = [256, 256], batch = 512.
	  Use checkpoint 126.
	* Trial 4 had mean reward = 1.86 with min ~1.3; LR = 3.39e-5, activation = relu, network = [300, 128, 64], batch = 2048.
	  Use checkpoint 97.

10/20/22:
* Time
  01:12 - 01:33 = 0:21
  17:15 - 18:31 = 1:16
  19:54 - 10:00 = 2:06
* Added the LCC penalty back into the reward method.  Made tuning run 211ad, all with a 3-layer network.
	* Trial 6 mean reward = 2.01, LR = 1.04e-4, activation = tanh, batch = 1024
	* Trial 11 mean reward = 1.75, LR = 1.99e-4, activation = tanh, batch = 1024
	* Trial 14 mean reward = 1.75, LR = 1.37e-5, activation = tanh, batch = 128; min reward above +1.5
	* Trial 18 mean reward = 1.76, LR = 1.67e-5, activation = tanh, batch = 128, min reward above +1.6
* Run 211ad showed
	* LC cmd penalty worked really well at keeping the LC command very close to zero
	* Accel was also really close to zero on trial 6, but averaged slightly positive so that speed rose throughout, but it never
	  reached the speed limit.  Trial 6 got only 0.796 completion reward, but gathered lots of time step rewards throughout.
	* Trial 11 had big accel, but its total reward was ~1.8 vs 2.2 for trial 6. So avg time step reward of ~0.09 is too high.
	* There is a strong desire to drive in lane 0, whether the initial lane is 0 or 1; LC occurs immediately if needed.
	* Max completion reward is only ~0.83 for the fastest possible travel time.  Needs to be increased.
* Added scaling of the action_space in the wrapper class to keep the NN output accel limited to [-1, 1] (it was in [-3, 3]).
  Modified the reward shape a bit to better emphasize completing the course as fast as possible.
* New tuning run (b5410) to finalize work on lane change issues.

10/21/22 Fri:
* Time
  16:52 - 19:37, int 23 = 2:22
  20:12 - 21:12 = 1:00
* One run from yesterday (b4510) succeeded, which was trial 1. Mean reward = 2.11, LR = 4.99e-5, activation = tanh, batch = 1024.
  Actions were well behaved, as desired, but accels were all small and tended to slow the car down to get more time step rewards.
	* Changed reward limits from [-1, 1] to [-2, 2], since the completion reward was being greatly clipped.
	* Reduced jerk penalty mult from 0.01 to 0.005.
	* Reduced time step reward from 0.005 to 0.002 (it was contributing >0.5 to the total episode reward)
	* Added penalty for exceeding speed limit (and increased obs space upper limit for speed substantially to allow an excess).
* Run fe601 with these changes produced no successful trials.  After observing one slightly promising run, made these changes:
	* Increased gamma from 0.99 to 0.999.
	* Reduced LR range a bit.
	* Added a HP to choose model's post_fcnet_activation between relu & tanh (was formerly fixed at relu).
* Another run with the above changes had no success either.  So I removed the speed limit penalty and created run 25ee5.
* During training, I continued to write the graphics code, but in separate copies of the files: inference.py,
  simple_highway_with_ramp.py, using the suffix "_new" on each one, so it won't affect the ongoing training.

10/22/22 Sat:
* Time
  10:18 - 11:00 = 0:42
  11:34 - 12:55 = 1:21
  13:50 - 15:01 = 1:11
  17:40 - 18:03 = 0:23
  19:55 - 21:25 = 1:30
* Runs from last night (92d01) that look promising are 3 (LR 9.36e-5, tanh/tanh, batch 1024) and 12 (LR 3.50e-5, relu/tanh, batch 512).
	* Run 3 ran close to 0 speed. When it finished, it collected 0 completion reward because it took 1176 time steps!
* Added penalty for slow speeds (normalized speed < 0.5), slightly increased penalty for jerk and slightly reduced penalty for 
  lane change command.
* Increased number of most_recent iterations to evaluate for stopping, and didn't stop if max reward is close to success threshold.
* Finished the graphics code for the initial roadway display and integrated it.
* Realized a MAJOR PROBLEM I have had: the training was always starting in lane 0. Also, since the vehicle initial conditions set by
  reset() were pretty limited (speed and location), it was often never seeing experiences for downtrack locations or for high speeds.
  Therefore, I modified reset() to randomize these, and the initial lane ID, over the full range of possible experiences.  It
  is not clear to me how to get Ray Tune to pass in random values for each episode (when reset() is called), so for now I'll
  depend on reset() to handle it.  I've added a "training" config option that opens up these random ranges; if it is False or
  undefined, the initial conditions will be as they were before.

10/23/22 Sun:
* Time
  15:11 - 15:30 = 0:19
  20:16 - 21:03 = 0:47
* Finally got one that seems to have learned: 10965 trial 10, used LR 5.80e-5, network [200, 100, 20] and output activation = tanh.
  However, this also did not perform well.  Accelerations are all over the place, and LC commands are as well.  Several inference
  runs failed to complete even half of the track.
* Added remaining code to the Graphics class in the inference program to do rudimentary display of ego vehicle driving on the track.
* Started a new training run using the DDPG algo instead of PPO (which I had been using thus far).  It seemed to produce some
  good results quickly, but the rewards never grew enough.  Started playing with the network structures more.
	* Some of the iterations are showing min rewards in the -200 to -300 range - how is this possible? Apparently, due to lots
	  of accumulated low-speed penalties.

10/24/22:
* time
  13:34 - 15:07 = 1:33
  19:57 - 22:31, int 15 = 2:19
* DDPG run from last night found some success!  Run 41f25, trial 3 used actor network of [256, 128] and LR = 9.6e-6, with critic
  network of [100, 16] and LR = 3.8e-4.  Inference run in lane 0 stayed there with small lane chg cmds the whole way, and gradually
  accelerated to max speed with no jerk penalties anywhere!  Running checkpoint 500 (after which the mean reward dropped a bit).
  Full run captured total episode reward of 1.54 taking 85 time steps.  Another inference run started with a low speed (0.16 scaled)
  and performed similarly, but, because of the low speed penalties at beginning, its total episode reward was only 0.34.
	* Trial 14 from that run also performed really well in inference, using actor network of [100, 16] and LR = 6.6e-6, and
	  critic network of [128, 32] and LR = 9.6e-4.
* Completed testing the graphics update method.  Had to modify the env wrapper to get access to both the scaled and uscaled obs.
* Added upper speed limit penalty and ran new training runs with DDPG, but not getting success. Max rewards tend to settle around 1.3
  while min rewards settle around -10 to -20, with means in the -5 to -10 range.  Maybe this settling is because noise gets removed
  too early?
* Started another run with:
	* Reduced upper speed limit penalty (0.2 at 1.2*speed limit)
	* Much larger noise decay schedule (from 90k timesteps to 900k), plus random noise for 20k timesteps.
	* Longer trials, up to 900 iters.

10/25/22:
* Time
  19:00 - 19:58 = 0:58
  21:45 - 23:22 = 1:37
* Last night's DDPG run did not succeed either.  From the four best ones, I see that the actor performed best with the largest
  network ([400, 100]), implying that maybe bigger would be better.  Also, it seemed to prefer LR ~2e-5.  The critic didn't seem
  to care as much about either of these params, so it is probably good with a smaller network.
* Created a new DDPG run, notably with a much larger replay buffer. Before, the default 50k experiences was used; now I am using
  1M experiences.  Also adjusted some HPs a bit.  Still no good.
* Run ac6fb: it seems that adding the upper speed limit penalty is causing it not to learn, so I removed that penalty and made no
  other changes for this run. This produced at least 2 successful trials!  It has therefore dawned on me that the problem is the
  magnitude of the penalties being imposed for approaching the speed limits.  They are way too big for a per-time step penalty,
  considering the other penalties are O(0.001) and these are O(0.1), especially when the offending situation is not possible to
  get out of in a single time step.  The negative reward piles up very quickly, discouraging any learning.
* Reduced the low & high speed penalties by about 2 orders of magnitude for new run fad55.

10/26/22:
* Time 20:04 - 21:12 = 1:08
* A few runs scored decent mean rewards (~0.9) very early, then they gradually dropped as the episodes went on. However, their
  early checkpoints perform pretty well.
	* It is willing to accept a penalty of ~0.003 for low speed and ln chg cmd (each). But it doesn't like a large jerk
	  penalty at all.  Seems to be willing to accept the low speed penalty in order to pick up more existence reward (0.005).
	* In another run that started much faster, it was willing to accept a high speed penalty of up to 0.014 for the entire
	  run of 100 steps, in order to pick up 1.14 points for completing the run fast.
* Changed rewards so that
	* Reduced existence reward to 0.003 (was 0.005)
	* jerk penalty maxes out at 0.006 (was much higher)
	* low speed penalty maxes out at 0.02 (was 0.01)
	* high speed penalty at 1.2x speed limit is 0.02 (was 0.01)
	* Reduced success threshold to 1.1 (was 1.2)

10/27/22:
* Time
  09:07 - 09:33 = 0:26
  11:53 - 12:25 = 0:32
  14:29 - 15:07 = 0:38
  16:29 - 16:51 = 0:22
  21:00 - 22:47 = 1:47
* Got a few runs whose mean reward peak came very early, and only hit ~0.2.  Common to accept lots of ongoing penalty for low
  speeds, but kept very gradual & small accels.
* Reduced jerk penalty.  Also removed the time step existence reward and reshaped the completion reward to drop off faster for
  slow traversal.
* Fixed defect in steps_since_reset counter initialization, which was causing success reward to be less than it should. Also
  set up new run with wider exploration of actor LR and noise params.
* Added condition to stop logic in case mean reward is low but max is above threshold; if mean is near the min than stop it.
* Turned off all lane change command penalties and ignore LC command coming into step() so it only has to learn about speed
  control.
  Still got no good results. This time each of the min, mean & max reward curves was almost flat, except for some noise, with
  the mean centered aroune 0.2, which is way below what it should be.  Running inference on one of the more successful trials,
  I see that it is on the gas all the way, maxing out high speed penalties, which accumulated to ~2x the completion reward.
  This doesn't make sense.  I therefore think it is learning that max speed is the best policy because each episode begins
  at a random downtrack location, so some of them are gathering close to 1.5 completion points for going very fast for only
  a few time steps.  Therefore, I changed the restart() method to always initialize training runs at the very beginning of
  the track so it has to drive the entire track to get a completion reward.
	* Debugging statments reveal that incoming accel actions, even at the very beginning of a training trial, are
	  highly correlated throughout the episode - mostly in the 0.8 - 1.2 m/s^2 range! But it is completing episodes
	  regularly very early.  It quickly learns to max out accel (+3.0) and minimize number of time steps, and never gets
	  to explore what happens below the speed limit.
* Replaced the OU noise model with Gaussian using sigma = 1.0 m/s^2 at the beginning (gradually annealed).  I confirmed that
  it initially produces accels all over the place.  However, it still quickly learns to push the accel to the max throughout
  the episode.

10/28/22:
* Time
  19:28 - 21:41 int 15 = 1:58
* Walked through a few series of manually specified accelerations through episodes in the new environment loop tester.  It
  appears all the rewards make sense.  Further, running a full episode at speed just below the speed limit gives a much
  better episode reward than going full throttle all the way through.
* Changed the hidden layer activations from relu to tanh for both actor & critic.  No improvement.
* Reduced the lower limit of the actor LR range.  Started to see some hope at the low end.  Need to go down into the e-7 area.
* Turned down the Gaussian noise sigma from 1.0 to 0.1.  No noticeable effect on the reward plots.
	* Inference on the best one of these (episode reward max ~0.8) showed that it seems to be learning to reduce its
	  accelerations (held close to 0.2), but it still lets speeds max out, so maybe slowing down the LR more will allow
	  it to discover the sweet spot.
* Reduced both actor & critic LRs even more.
* TIMING: on laptop battery, one episode of 145 iterations on 1 worker/1 env took 2:51.
	  1 worker/4 env took 3:01 on battery (took 3:09 on AC power)

10/29/22:
* Time
  09:00 - 09:18 = 0:18
  11:29 - 12:08 = 0:39
  16:15 - 18:23 = 2:08
* Runs from last night with much lower LRs kept the max reward steady > 2, and even mean rewards stayed steady instead of
  dropping, but still < -1, since the mins never improved.  It does appear that gamma = 0.999 is important, and a more
  narrow range of LRs is the sweet spot.
* More tuning with tau.  Doesn't make a big difference. Nor does adding more Gaussian noise.
* Switched to TD3 algorithm, using the defaults suggested in the algo manual (they aren't provided to copy).
* Took a break to train the racecar project, which is a simplified version of this, only trying to drive straight down
  a lane as fast as possible, but while respecting the speed limit.

10/30/22:
* Time
  15:08 - 16:09 = 1:01
  16:48 - 17:53 = 1:05
  21:33 - 22:56 = 1:23
* Racecar toy taught me that it is important to keep the cumulative amount of possible penalties (over the episode) on the
  same order of magnitude as the completion reward.  I had had them about 2 orders of magnitude larger.  I also suspect that
  it will be beneficial to just let the system train a lot longer than I have, even though there hasn't been any clear
  progress in a few hundred iterations.
* Applying these lessons to the cda0 project brought some quick success for the limited case of just speed control with TD3.
	* Critic was [256, 32] for all trials.
	* Found that actor network of [128, 16] is definitely too small.
	* Best performers had actor of [256, 32].  They all learned quickly and reward curves were smooth.
	* Actor network of [512, 128, 32] struggled to learn, but some trials did well. Best ones were the lowest LRs
	  (1.2e-6 for actor and 4.7e-6 for critic)
	* I was unable to get checkpoints to load for inference engine - error about differing state dict param sets,
	  even though I verified the network structures were specified the same.
	* Max rewards were smaller than I had hoped (but I can't see exactly what's going on due to no inference).
* New training run using DDPG and using lane change control also (just lanes 0 & 1).  Changed reset() to randomize the
  vehicle's initial location anywhere along the lane instead of just at the beginning (it was learning to just come to a
  stop to avoid the perpetual negative rewards).
	* Five of the 15 trials appear to have succeeded! Run 8c03d using critic net [256,32]. Results are in the
	  cda0-l01-free dir.
	* Trial 0 had a long & jagged learning curve, but got there. Actor [256, 32], actor LR = 2.5e-5, critic LR = 3.1e-5,
	  tau = 0.005.
	* Trial 2 had actor [256, 32], actor LR = 8.2e-6, critic LR = 1.6e-5, tau = 0.005.
	* Trial 10 had actor [256, 32], actor LR = 7.8e-7, critic LR = 1.1e-6, tau = 0.001.
	* Trial 11 had the fastest learniing curve, with actor [256, 32], actor LR = 9.6e-7, critic LR = 7.7e-5,
	  tau = 0.005.  In inference it used modestly high accel all the time and ignored the speed penalty. Rewards ~0.94.
	* Trial 12 had actor [256, 32], actor LR = 4.7e-6, critic LR = 1.8e-5, tau = 0.001.
	* None of the trials with an actor net of [512, 64] was even close.
* Next run includes the following mods:
	* Increased success threshold from 1.0 to 1.1, which is not reachable by going full throttle all the time.
	* Increased minimum required iterations to 300 to ensure we have plenty of settling time.
	* Added penalty for LC cmd values near +/-0.5. 
	* Doubled the penalty for high speed violation (previously maxed out at 0.001). 
	* Results of this run (be9bc) showed that the [200, 20] actor network could achieve success, but not as easily.
	  Also, inference of a couple winners showed they still prefer full speed and a lower episode reward.
* Next run (23f2e) doubled the high speed penalty again.

10/31/22 Mon:
* Time
  16:18 - 17:00 = 1:42
  20:50 - 21:15 = 0:25
* Results of 23f2e run:
	* Several trials reached episode reward between 1.0 and 1.05 quickly, and mostly stayed there.
	* None reached the success threshold of 1.1.
	* Best trials were 4, 7, 9, 13.  Others that reached 1.0 but had some downward spikes: 1, 8, 12
	* It appears that probability of success favors the [256, 32] network over the [200, 20] actor; also the larger
	  tau (0.005) seems more favored.  Also, as expected, a ratio of actor LR / critic LR ~0.1 seems best.
	* Inference shows that these models still want to use a high accel and are very reluctant to change its value.
* New run with following changes (3b68a):
	* Trying larger 2-layer networks (the 200 last time wanted higher accels)
	* Eliminate the jerk penalty to encourage large changes in accel.
	* Double the high speed penalty (it was maxing at 0.0041).
	* Use OU noise to be more realistic in generating variations in accel.
* Results: successful trials have mean rewards plateauing ~0.8, despite max rewards consistently being > 1.4. 
  Inference run still accelerates to max speed and stays there.  Lane chg commands remain close to 0, however.
* New run with following changes
	* Tuning noise magnitude
	* Tuning with larger actor network (512 nodes in first layer)
	* Tuning with larger choice of critic network
	* Doubled the high speed penalty again to give max of 0.016

11/1/22 Tue:
* Time
  04:07 - 04:47 = 0:40
  11:46 - 12:20 = 0:34
  15:56 - 16:32 = 0:36
  19:01 - 20:03 = 1:02
* Analysis of prev run:
	* No cases were successful.  However, 3 of them quickly reached mean reward ~0.5 and stayed there.  Then
	  2 of them fairly quickly (~500k steps) reached mean reward ~0.2 then slowly climbed to reach 0.8 after
	  7M steps.  It appears they would keep going with further training.
	* The two promising ones had both networks at [512, 64], noise in use (for 3M steps), and a LR ratio of
	  actor/critic close to 0.1 with actor LR between 2e-7 and 5e-7.  In each case their min reward stayed around
	  -1 and max stayed at 1.48.
	* On inference, they both had learned that small positive accel is the answer, and kept the LC cmd quite
	  small as well.  They did not learn to maximize speed within the safe range, however.
	* The four that got almost as high results showed a similar LR ratio, and 3/4 had the larger critic network.
* Modified reward code to randomly cancel the episode completion bonus if high speed violation occurs; probability
  of cancellation is proportional to the amount of excess speed involved in that time step.
	* Similar to previous run, a couple cases gradually increased mean reward (max ~0.7).  These were 0, 2, 3
	  (for a while, then dropped off).  These all had actor network of [512, 64] and critic network of [768, 80]
	  and similar LRs: actor ~3e-7, critic ~2e-5.
	* Inference on two of them showed same pattern of sticking to very small, positive accelerations, regardless
	  of initial speed, and letting it run into the high speed violation.
* Modified reset() method to change initial position of the vehicle during training.  It had been allowed to start
  anywhere along the route, but I feel that is encouraging it to go for the big score and ignore speed limits, and
  therefore, not worry much about accel.  As iterations progress, the initial position will gradually be squeezed
  toward the beginning of the track, forcing it to train for longer episodes.  Also adjusted the probability of
  cancelling the completion bonus upward (to worst case 4%).
	* Most cases resulted in flat mean reward curves plateauing at ~-0.2, so no good.  Three reached positive
	  territory, however.
	* All 3 best trials used actor of [512, 64] at LR between 2e-7 and 7e-7, and critic of [768, 80] at LR
	  between 2e-5 and 5e-5.
	* The biggest peak mean reward (trial 14) was 0.3, but it eventually tailed off to < 0 (after 5M steps).
	* The worst performing of the 3 (trial 12) peaked ~0.1, then quickly dropped to < 0 after 2.5M steps.
	* Inference runs show that these didn't perform any better than the previous training run.  They learned
	  to keep accel small, but have no idea that speeding up to the speed limit is advantageous, or that
	  slowing down if above it is good.
	* I confirmed that actions coming into the step() method tend to cover the full range of possible values,
	  at least early in a training run.
* Modified reward code to add an accel bonus if recent (4 steps) avg speed > speed limit and avg accel over that
  period is < 0, and vice versa for speeds below speed limit (exceept for a deadband).  Bonus increases with
  larger speed difference from the limit and with larger acceleration magnitude.

11/2/22:
* Time
  10:37 - 10:59 = 0:22
  12:51 - 13:38 = 0:47
  19:51 - 20:32 = 0:41
* Analyzed run from last night:
	* Pretty similar performance as before - 3 runs reached mean reward > 0, maxing ~0.3. 
	* Inference still shows a desire for accels very close to 0 and a slow change to it.  However, one run
	  demonstrated a slight response to the high speed penalty, where speed got into that zone, then accels
	  turned negative and returned it below speed limit.  It took many time steps, however.
* Modified reward code to double the probability of eliminating the completion reward if high speed, and
  doubled the accel bonus value.  
	* Running inference on some very early checkpoints (2-20 iterations) shows that already the accels and
	  LC cmds are very small.  This makes me wonder if there is a scaling problem.
	* Adding print statement during training run shows that scaling is not a problem; all calcs seem proper.
	  It is just learning very quickly (in first 20 iters) to keep accels close to zero.  I now suspect
	  that this may be due to too much smoothing by training in large batches.  Also the small time step
	  may be having some smoothing impact.
* Next set of mods:
	* Tuning with much smaller batch sizes (down to 8)
	* Simplified accel bonus calcs to just be based on current time step, not history.
* Analyzed above run (0c04c):
	* All cases had a mean reward > 0, but none of them exceeded 0.8 despite each one having a max > 1.3.
	* Runs 0 & 1 peaked quickly then dropped rapidly.  Batch sizes were 1024 and 128, respectively.
	* Run 2 took the longest to peak (6M+ steps) but also had the lowest max (as low as 1.25 at 7M). Its
	  batch size = 128.  LR among the smallest & largest at 1.2e-7 for actor and 9.3e-5 for critic.
	* Run 3 also peaked fairly quickly and dropped off a lot. Batch = 128.
	* Run 4 was a slow mover but peaked nicely. Batch = 16, actor LR 1.7e-7, one of the lowest.
	* Run 7 was lowest mean peak (but highest max), and dropped away very quickly.  The only batch = 8.
	* Runs 2, 5, 4, 13 had the lowest actor LRs, and their critic LRs were widely different. They all
	  showed gradual peak then tail-off of mean, plus max started high (~1.55), dipped, then climbed again
	  until its end.  The dip was lower for those whose means took longer to peak.  Once means tail off,
	  the max climbs again.  These had batch = 128, 16, 1024, 1024.  It seems they may have continued to
	  improve with more time.
	* Inference performed similar to previous runs - accel very small and slow to change, but they are
	  starting to see the correct directions to move.
	* It doesn't appear that small batch size has a noticeable effect.  Best bet seems to be LR ~1e-7 for
	  actor and 5e-5 to 9e-5 for critic, then let it run a lot longer.
* New run:
	* Using new StopLong class for the stopper, which pretty much lets it run to max iterations unless
	  the max reward is a failure.  Also extended max iterations to 2000.
	* Magnified the noise.
	* Tightened the LR ranges, and moved the actor lower and critic higher.
	* Doubled reward bonus for correct accel action.

11/3/22:
* Time
  10:31 - 10:59 = 0:28
  18:00 - 19:17 = 1:17
  21:13 - ?
* Analysis of last night's run
	* Some runs got peak mean reward of close to 0.7, similar to previous. Most has max rewards >= 2.
	* Case 1 and 7 started very slowly, then gradually increased mean reward; the only two that didn't
	  drop off within the 2000 iterations.  Peak value of mean was ~0.4.  Also, their max values were on
	  the lower side (~1), then began to climb towards the end.  They are ripe for additional training. 
	  These both had batch = 8 and 2 of the lowest actor LRs (7e-8 and 9e-8), with the same critic LR
	  of 9.8e-5.
	* Case 6 is also interesting, as possibly the best performing of the others, with peak rewards
	  ~8M steps.
	* Inference results are similar to previous, however. Not satisfying.
* Mods for a new run:
	* Check for even smaller LRs for actor, larger for critic.  Also, throw in a couple really big ones.
	* Try batch size = 4.
    ***** Can config params get passed into the env object for each run?  Yes - the configs are passed in to
	  the init method, and it is called by each worker at the beginning of an iteration.  Any values
	  will be held constant throughout all episodes of that iteration (just like extrinsic items are,
	  such as LR).  This could be a means to schedule gradual changes in individual reward penalties or
	  bonuses, or even in environment dynamics, such as taking off training wheels (removing limits on
	  action outputs).  It could also be used to randomize some env constants to effectively provide
	  data augmentation (e.g. change friction coefficients, control response times, control biases, etc).
* Analysis of above run (e9151):
	* Case 2 looks maybe promising in late time steps - long, steady mean reward, probably the highest
	  mean and max reward near the 12.5M step mark.  Actor LR = 1e-5, critic LR = 9e-5, batch = 8.
	* Case 3 very similar to 2 but much less erratic reward plots.  Actor LR = 4e-8, critic LR = 9e-5,
	  batch = 16.
	* Case 5 no good but had an interesting, huge spike in both mean and max rewards at ~2M steps, then
	  terminated soon after.  Is this where noise ended?
	* None of the 3 cases with batch = 4 was able to hold a mean reward > 0.  At least one had what I
	  believe is a good LR ratio.
	* Inference runs (of case 2) show that it is definitely reacting to the high and low speed penalties
	  and/or accel rewards, but its reaction time is many seconds, not one or two time steps as I would
   **     like to see.  Is it possible that the unused historical accel values in the obs vector arc
	  contributing to that?  Hard to believe, but maybe.  No - experiment shows it is not.
   **   * Other things to try:  play with size of completion reward (even make it zero at all times) or
	  at least ratio of completion reward to penalty magnitudes (maybe the all need to be closer to 1?).
	  Try PPO or another algo to get more dynamic action response to changing conditions.
	  Try training with a slow-down zone in the road to force larger accels.

Took a few days off and played with the simpler Racecar project to get back to basics. This is in the
projects/cda0_copy dir.  An even simpler variant, Car, is in the projects/copy2 dir, and in Github under
repo name simple-car.  Used PPO as the training algo.

LESSONS LEARNED from simple-car:
* I was able to train a car to drive a straight lane without traffic, as fast as possible while obeying the
  speed limit.
* Keeping time step rewards large enough to present noticeable impacts on derivatives is important (values
  in O(1e-3) or O(1e-4) were not doing it). Final time step's completion reward was O(10) and incremental
  values were O(0.01) for 150 time step episodes.
* Shaping all rewards/penalties as smoothly differentiable seems to be important. Using quadratic function
  of distance from target value is better than piecewise linear.
* Start with really small NN structure.  I was getting good results with a [16, 4] FC network.
* HP tuning takes a LOT of time.  Most of them don't have much impact, but LR is very sensitive.
* May need to get several trials of ~same LR before finding one that works with initial weights distro.
* May need to let training run a long time so it can gradually converge after several chaotic dips in 
  performance.
* Important to get a good balance of noise, which probably needs to be extended throughout most of the
  trial.
* Be sure noise is explicitly turned off during inference runs!  Using Ray algorithms may bring in unseen
  config settings that turns it on by default (e.g. PPO's "explore" flag).
* Ray makes it very difficult to continue training from a Tuner checkpoint.  For insights, see
	* https://discuss.ray.io/t/save-and-reuse-checkpoints-in-ray-2-0-version/8169
	* https://discuss.ray.io/t/correct-way-of-using-tuner-restore/8247
	* https://discuss.ray.io/t/retraining-a-loaded-checkpoint-using-tuner-fit-with-different-config/7994/7

11/19/22:
* Time recorded directly on dashboard
* Merged the reward and tuning code from the simple Car code into the cda0 code and ran a tuning run there in
  attempt to duplicate the success of training a drive in a straight lane with no other traffic around.  The
  only difference is that the cda0 environment now involves all the other observation elements and a 2D action
  vector (although the 2nd element is not yet used).
	* Results: two of the first 4 trials achieved mean rewards > 9.  Running them in inference (with no
	  noise) starting in either lane 0 or 1 showed good performance.  However, it seemed to be a little
	  too afraid of jerk, and happier to go slowly and take a lower completion reward.
	* The one trial that had random_start_dist turned off never went anywhere, so it seems this is essential.
* Trying a new run with the low speed penalty turned on and a little smaller jerk penalty.
	* This was still really tame in terms of acceleration & jerk, so accepted some slower-than-desired
	  solutions, with very slow, smooth accels.
* New run with ligher jerk penalty again, by 10x more, and with 2.5x more low-speed penalty.
	* Trial 00006 performed beautifully! Smart accel at the beginning, and smoothly leveled off when speed
	  approached posted limit, then stayed there.  Exactly what I wanted.  This is saved as trial
	  PPO_SimpleHighwayRampWrapper_1656e_00006 under ray_results/cda0-l01-free, and against code committed
	  on 11/20, commit fe894d0.
*****
* Straight lane performance is now complete.  Time to move on.

11/20/22:
* Time recorded in dashboard
* Ran the cda0 tuning code with all 3 lanes as starting options, meaning in lane 2 the agent has to learn to change
  lanes in order to finish.  Results are now being recorded in ray_results/cda0.
	* Results were poor.  None of the 15 trials got a mean reward > -8 or so, although the max reward was
	  consistently ~ +16.
	* One trial aborted due to a Ray error, but was looking like it could possibly break out to higher ground
	  than the others.  Its LR = 4.5e-5.

12/4/22:
* Time recorded in dashboard
* Narrowed the LR range a bit and increased the noise magnitude (stddev) from 0.3 to 0.5 (Gaussian).

12/5/22:
* Results from yesterday's run:
	* Nothing got above mean reward of 0, but one trial came close, peaking twice around -8 before falling off.
	  Running inference on this one's peak checkpoint...
	* Starting in lane 0, it ran smoothly, but at a nearly constant, small positive accel, thus hitting top
	  speed eventually, and taking a penalty of 0.4 per time step.  Total score = 10.4, but it could have done
	  a lot better.
	* Starting in lane 1, it immediately changed to lane 0, taking a 0.05 penalty for that (trivial), then
	  stayed there decisively. Accel performance was similar to lane 0.
	* Starting in lane 2, it tried to make an illegal lane change in first time step, so crashed.  This is
	  repeatable 5 times, but with slightly varying action outputs.
* Mods for next run:
	* reset() printing lane selection to verify that it is training in lane 2. Verified it is choosing all 3
	  lanes randomly, so turned this off again to avoid clutter.
	* Tuning with choice of random seeds, based on a recent article I read.
	* Increased max LR quite a bit, since rewards tend to be small (O(1)), and therefore gradient is not large.
	* Increased noise slightly, from 0.5 to 0.6 magnitude of sigma.

12/7/22:
* Results of prev run:
	* Trial 0 had LR = 7.99e-6; it drives lane 2 all the way, but runs off the end rather than change lanes.
	* Trial 7 had LR = 1.62e-5; make immediate lane chg in lane 2, so goes off road.
	* Trial 10 had LR = 1.9e-3
	* I verified that both the raw & scaled obs vectors are correct.
	* Accel is stubbornly small, positive throughout all runs, regardless of speed & reward.
* Ideas to try (from reviewing above history):
	* Reduce size of NN
	* Play with LR schedule
	* Turn off random start distance during training (force it to complete full route every time)
	* Train in only lane 2 to see if it can at least learn that one
	* Turn off jerk penalty
	* Play with noise magnitude
* Mods for next run:
	* Forcing lane ID to be 2 always (in reset())
	* Commented out jerk penalty
	* Results:
		* 4 trials had mean reward peaks very close to 0 (> -10), but dropped off fast afterward.  These
		  had LR of 1.4e-4, 1.4e-4 and 5.0e-5.
		* All had mean reward that stayed at -50 for a long time, then several started climbing fast
		  around 500k to 600k steps after max reward suddenly jumped from -50 to +10.
		* No min reward ever exceeded -50.
		* Running trial 3 inference made it to the end!  It had several lane changes, which is fine.
		* Accelerations were somewhat sporadic, but much more aggressive at times.
		* Found a defect in geometry code that repositioned the vehicle backwards one step after making
		  the lane change.
* Mods for next run:
	* Fixed distance calc during lane change. I can't believe this had a significant impact on training,
	  although it did make the distance signal discontinuous, which could have had some effect.
	* Added tuning options for NN size.
	* Trial 3 performed well, with a mean reward hovering close to +10 for many iterations before finally
	  tanking.  Inference runs on it, in lane 2, performed ideally, holding the speed limit and changing lane
	  toward the end of lane 2.  When started in lane 1, it immediately tried to change lanes left too much.
	  This one had a NN structure of [64, 24] and LR = 7.8e-5.
	* None of the others achieved a promising mean reward, so I don't feel I'm really finding the sweet spot.

12/11/22:
* Mods for next run, which is still only training on lane 2 start:
	* Fixed the NN structure at [64, 24] and tuned for noise magnitude (still decaying to 0.04 at 1M steps).
	* Two very promising trials were terminated prematurely due to stop criteria taking slopes over way too
	  many iterations.  I need to reduce this, and also print out more details when it decides to stop.
	* Three of the first 11 trials peaked between 0 and +10 (mean reward); not great performance, but decent.
	  Inference on two of these shows good performance.  Their LRs were 7.5e-5, 1.1e-4, 9.5e-4, and they
	  used noise magnitude (stddev) of 0.61, 0.25, 0.17, respectively. So I believe I have this lane change
	  nailed.

* Mods for next run:
	* Reduced the avg_over_latest param from 300 to 60 iterations, because I believe this is iters, not
	  steps. I added better output on stop condition to help confirm this.
	* Zeroing in tuning to LRs in the range of success above, as well as noise magnitudes in the lower end
	  of that range.  
	* Reinstate all 3 lanes as candidate initial conditions (mod to reset()).

12/12/22:
	* Trials 6 & 8 peaked at mean reward ~5. Inference run shows it is pretty good, but willing to accept a lot
	  of speed penalty, with only very tiny adjustments to accel.  They both performed extraneous lane change
	  to lane 0 if starting in 1 or 2, which cost an extra ~0.2 penalty point.
	* Jerk penalty has been turned off for some time, so it is learning smooth acceleration from other means.
* Next run:
	* Increased size of NN from [64, 24] to [64, 40]
	* Turned off randomized start distance during training (forcing all starts to be at beginning of lane).
	* The first 6 trials here were lousy (one peaked at 0 and one peaked at -5), indicating that maybe
	  randomized start distance is still needed, so I aborted the run.

* Next run:
	* Increased num workers from 8 to 12, keeping the rollout fragment length = 200, so train batch size
	  increased to 2400.  Hoping things will move faster now.
	* Turned the randomized start distance back on.

12/13/22:
	* It doesn't appear that the randomized start distance had any particular effect, which was expected,
	  since it had already learned to travel the full length of the course.
	* A couple of the trials peaked above 0, and one got to +10 for mean reward. However, inference on it
	  showed pretty loose speed control.  It had no problem reaching way into the penalty areas for extended
	  periods.  Although it did seem to sense it didn't want to be there, the jerk went negative as soon as
	  it entered the high speed area, but it was so small that the accel took a long time to reverse the
	  incursion. With jerk penalty off, I don't understand why it won't change accels more quickly.

* Next run:
	* Based on successful straight-lane results on 11/19, with the more aggressive jerk performance, I am
	  going back to that [64, 48, 8] NN structure.

12/14/22:
	* This run was worse than the previous. Only one trial reached above 0 mean reward, but stayed < 5.
	  It had LR = 8.8e-5 and noise stddev = 0.28.
	* Like pretty much all trials to date, good or bad, the mean reward tends to climb to some peak,
	  usually around 400k to 600k steps, then it falls off, sometimes dramatically. I feel like LR
	  annealing is going to be important to get past this problem and allow things to keep learning.

* Next run:
	* Figured out how to add LR annealing to the PPO params, so did that, going from 2e-4 to 2e-6 over
	  the first 800k steps.  Did not try to make this a tuned param at this time.

12/15/22:
	* The chosen schedule did not perform well at all.  3 trials peaked just below 0, but all exhibited
	  some amount of pull-back after a fairly short peak.  Trials that tended to stay the most flat
	  after peaking were 2, 4, 5, 7, 11, 13. There is no correlation here with noise magnitude, as these
	  span the full range allowed, as do the trials not listed here.

* Next run:
	* Changed the LR schedule a bit, generally moving it to lower values (about 2x change), with an
	  extra breakpoint.
	* AI: I cannot see the exact LR being used in the log for any given time step. It would be good to
	  add a print statement into the PPO code to make that visible.

12/16/22:
	* Three trials peaked above 0 but below +10. Inference on 2 of them showed close to constant,
	  small accel throughout, but got decent rewards in lanes 0 & 1 (~6-9); but when starting in lane 2
	  it continually either slowed to a stop or drove off the end of the lane. No good! The third trial
	  performed about the same in lanes 0 & 1, but in lane 2 it always did an illegal lane change in
	  the first time step.  Clearly, nobody has learned how to drive lane 2 here.  This explains the
	  persistent plots of -50 to -80 in the min rewards arena. It never exceeds -50.  Two of these
	  trials had large noise magnitude (0.48 and 0.46), seeming to indicate that larger noise is good.

* Next run:
	* It appears that lane 2 performance was good when that was the only thing trained, but when the
	  other lanes were thrown in it never learned well.  Possible solutions:
		* Force the training to select lane 2 more often that totally random choice.
		* More iterations
		* Larger NN to accommodate the additional stuff it needs to learn.
		* Use more noise
	* I will try to make the NN wider instead of deep.  Changing from [64, 48, 8] to [128, 50].
	* Increased noise magnitude to the range of [0.4, 0.6] and stretched out its schedule to fully
	  decay to 0.04 at 1.6M steps (which is typically where the latest runs have been ending).
	* Modified reset() to train on lane 2 50% of the time and lanes 0 and 1 25% each.
	* Added stop condition in the StopLogic class to terminate if the mean reward degrades at least
	  x (currently using 25%) below its peak in a given trial.

12/17/22:
	* The 6 trials complete so far are looking like better trends, steadily climbing toward zero,
	  but some of them are being cut off prematurely by a defective stopper, so I aborted.

* Next run:
	* Fixed the stopper defect (in the new code I added yesterday).
	* Adjusted the LR schedule so it doesn't drop off so fast, based on where the reward curve was
	  starting to flatten out.
	* Adjusted the long-term noise to be a bit higher, and increased the initial range somewhat.
	* Trials 2 and 8 peaked between +8 and +10 on mean rewards, and performed really well in
	  inference on all lanes.  Noise on these was 0.65 and 0.41, respectively.  Accels were a
	  little jerky at times, but there is no jerk penalty, so it's to be expected.  Agent tends to
	  like the lower speeds a bit, so it would be worth increasing that penalty a little and
	  narrowing its deadband as well, but not a major problem.  Also, the accels were more
	  aggressive than I've seen in recent past, so I guess the extra neurons made that possible.
	  If that's the case, then maybe a few more still could be more useful.
	* This is run 53a0c, and I am leaving it stored in ~/ray_results/cda0-solo.
 **	* AI: tweak the completion reward to drop off a little faster with the number of time steps.
	  There is hardly a noticeable difference between 130 and 170 time steps, so not much motivation
	  to speed it up.
 ****	* I believe I have found what I'm looking for in the solo vehicle department.  Time to move on
	  and build a version that can handle other traffic on the roadway.
 **	* Lesson:  if the NN feels too small, try to add width before adding depth.


>>>>>


12/18/22:

DRIVING IN TRAFFIC

The code is essentially already in place to start running 3 neighbor vehicles on the track along with
the ego vehicle (AI agent).  I made a few small changes, noted below, to turn that code on.  It will
run 3 vehicles at constant speed in lane 1.  That speed can be varied for each trial by Tune, as can
their starting downtrack distance.  However, for the first run, I am leaving them constant.  These
vehicles will always all drive the same speed, so they won't crash into each other.  They will remain
2 vehicle lengths apart, which leaves a 1-vehicle bumper-to-bumper gap between them, not enough for
the ego vehicle to slide into without registering a crash.  Hopefully, this will force the ego vehicle
to either speed up to get in front or slow down to get behind if it is trying to change lanes while
they are in the way.  For now the ego vehicle will be started randomly, as before, in any lane and
at any location and speed.  Therefore, it will often never see a conflict with the neighbors.
	* Changed completion reward from parabolic to linear, and made it degrade to 0 sooner (300
	  steps vice the previous 600 steps).
	* Changed penalties for failed episodes to give a little less weight to those that endedd in
	  an off-road (-40) or stopping in the road (-30), while a crash with another vehicle is still
	  worth -50 points.
	* Modified reset() to give the neighbor vehicles a non-zero starting speed and location, which
	  can be configured.

* First run, set neighbor speed constant at 29.1 m/s and neighbor location constant at 320 m, which
gives n3 the same travel distance to the merge point as a vehicle would have if starting at the
beginning of lane 2.  This feels like reasonably good chances of forcing a merge crash situation.
I am staying with the same NN structure and other tuning params that were successful yesterday in the
solo vehicle training.
	* First several trials did decently, peaking between -20 and -10.
	* Five of the first trials died with an error.
	* During this run I enhanced the inference program to display the neighbor vehicles as well.
	  It became obvious through this that they form a tiny target, so training episodes will
	  probably normally miss them altogether, thus the agent won't have much opportunity to learn
	  anything about deconfliction.

* Next run:
	* Made the vehicles much longer (40 m, which is about 2x the lenth of a semi), and started
	  them farther apart, so that they will present a much larger barrier to lane change from
	  lane 2.
	* I adjusted the long end of the noise schedule so that it doesn't die off so soon (now goes
	  to 0.1 at 2M steps).
	* All trials progressed well, and all leveled out without any big drops.  But their plateau
	  was around -18 to -8, so no successes.  Some inference runs with the new graphics show that
	  40 m is too long for the vehicles in this scenario, as the 3 neighbors can completely
	  block the merge area.

* Next run:
	* Reduced vehicle length to 20 m.  Also reduced the neighbor initial spacing in reset()
	  from 4 lengths to 3.  At this length & spacing they can block about half of the merge area.
	* Realized that the randomized start distance is being limited on a schedule that expires
	  after 400k episodes.  This is going to increase more slowly than the time step count, but
	  I increased the limit to 800k episodes to see what will happen.
	* Fixed a defect in the crash detection logic.

12/20/22:
	* Results here are similar to the previous run, with all trials ending in the -20 to -10
	  range.

* Next run:
	* Increased NN size to [256, 64]

12/21/22:
	* No improvement in performance. Most trials ended with mean reward between -20 and -10.
	  Two of them peaked around -7.

* Next run:
	* Changed reset() calc of max_distance for randomized start distance.  I confirmed that it
	  is based on episode counts, which seldom exceed 3000, but it was stretching it out over
	  800k episodes.  I pulled that back in to schedule the reduction over 2000 episodes, so
	  that my later episodes will be forced to run virtually the whole track.
	* Reduced the initial LR somewhat.
	* Trying some 3-layer NN structures as a tuning variable.
	* One trial reached -22 for mean reward, but the others slowly crept upwards in the low
	  -30s before being terminated after ~1.2M steps due to the max reward falling to -30.  It
	  would be interesting to see one of these left to run for several million steps since it
	  is improving.
	* I noticed that these trials are running beyond 8000 episodes, so the max_distance param
	  is getting shortened too fast.

* Next run:
	* Changed the max_distance schedule to reduce over 8000 episodes.

12/22/22:
	* A few trials did the typical plateau between -20 and -10. One peaked slightly above -10
	  but couldn't hold it.  Several stayed in the -35 region the whole way, with max rewards
	  rapidly dropping down to -30.  These tended to be the 3-layer models.

* Next run:
	* Made a few adjustments to the stop logic to help struggling borderline cases continue.
	* Changed tuner to select from larger 2-layer models.
	* AI: I think what is really needed is some curriculum training where the model first gets
	  trained to navigate the track without traffic, then introduce traffic.  I haven't yet
	  figured out how to save a checkpoint from a tuning run, which would be maybe needed to
	  do that.

12/23/22:
	* Process died during trial 5.  Of the ones complete, only one peaked above -15 and one
	  reached above -20.

* Next run:
	* Added some rudimentary curriculum learning by using a new environment config param
	  to specify at what point neighbor vehicles will start being used (time step #).  Prior
	  to that episode, they will stay in their initial positions like before I started using
	  them.

12/26/22:
	* The best trial plateaued between -15 and -10.  In inference, its best checkpoint showed
	  a LOT of lane changing.

* Next run
	* Fixed a defect in reset() that was not turning on the neighbor vehicles for phase 2 of
	  the curriculum learning, so the agent never experienced neighbors in the previous run.
	* Improved curriculum training capability by allowing definition of multiple phases in
	  the StopLogic class.

12/29/22:
	* Didn't get any notably different results.

* Next run (96415)
	* Added an arg to StopLogic to let a trial run to max iterations unless it is a winner.
	  Ran two trials like this.
	* Early max rewards took a smooth slope downward from 10 to ~4, as before, but between
	  3M and 4M steps it suddenl spiked up to +10 and stayed near there. Also about that
	  point the mean reward started getting less smooth.  In one of the trials it had long
	  bursts up above -5.  These ran to 1800 iterations (~5.5M steps).
	* Inference on the best checkpoint (training mean reward ~0) showed decent performance
	  in the straight lanes, but it kept doing illegal lane changes early in lane 2.

* Next run (a0c3c)
	* Fixed a minor defect in reset() where it printed some statuses after they were cleared.
	* Extended the max iterations from 1800 to 2400, since it still looked like there was
	  some progress being made at that point.

12/30/22:
	* One of these two runs performed similarly to the "good" one previous, in that it peaked
	  at mean reward = 0, but its fluctuations looked like it could have benefitted from
	  more iterations.
	* In the log file I notice that each trial stopped after a few 100k time steps (400k
	  and 580k, respectively), due to reaching iteration limit.  This never triggered the
	  neighbor vehicles to turn on!  Therefore, all this training was for solo driving. I
	  suspect that having 12 jobs running in parallel caused this problem.  Ray is summing
	  all time steps from all workers to get the 7M or so on the plot, but each worker is
	  only contributing 1/12 of that.  But the threshold to turn on the neighbors is
	  assuming each env object goes all the way to 7M time steps.
 **	* AI - if all I've been training is solo driving, why is it so hard to get good rewards?
	  Need to compare to successful solo training for HPs.

* Next run (2784c)
	* Changed tuning program to only use 1 worker (was 12).
	* Extended iteration limit from 2400 to 3000.
	* Now that all time steps are happending on 1 worker, it is transitioning to using
	  neighbor vehicles as expected, beginning at 1.2M steps.
	* In both trials the max reward took a huge step down at 1.2M steps (to around -30); in
	  one of them it quickly recovered to  around -5, but in the other it stayed at -30 for
	  the remainder.
	* My num crash tracker was being reinitialized wrong, but there is reason to believe
	  that no crashes have been detected, which is bothersome.
 **	* It really bugs me that a trial progresses at virtually the same speed whether it is
	  using 12 workers or 1.  Each worker is assigned 1 cpu, 1 env and 0 gpu.  The eval
	  worker has 1 cpu, 2 env and 1 gpu (the full enchilada).  I need to spend time
	  playing with various combinations to understand how to improve performance.

* Next run
	* Increased terminal LR (1e-6) to apply at 7M steps instead of 3M to be a little more
	  like the solo vehicle success on 12/17.
	* Added a new tuning option to use a NN of [512, 64] as one of the options.
	* Changed the final noise magnitude from 0.2 to 0.1 (still ocurring at 4M steps) to
	  be more like the solo vehicle success.
 	* The first litmus test needs to be that the rewards look acceptable at the 1.2M step
	  mark, indicating that it has learned to drive solo before adding the neighbors.
	* Enhanced StopLogic to use a let_it_run flag for each phase, so that we don't waste
	  time if a trial can't achieve good solo driving first.  I now have 3 curriculum
	  phases:
		0 = 1M steps to learn solo driving without aborting
		1 = 200k steps to allow stop logic to evaluate rewards and abort while
		    still driving solo
		2 = xM more steps with neighbors in motion to learn driving in traffic.
	* Moved from 1 gpu on local worker and 0 on rollout workers to 0.25 on local
	  workers and 0.5 on the rollout worker to see if it changes overall trial time
	  (current pace is close to 1M steps/hr).  I immediately found this doesn't work, as
	  Ray hung before any trials ever started.  So I moved these configs back to the way
	  they were before.
	* Realized a defect in the phase management design, where the min timesteps is doing
	  double duty as also defining the phase boundaries, so it will never exceed the current
	  phase's min timesteps, so never trigger an early stop.

* Next run (10 trials, ID 3db63)
	* Fixed StopLogic defect by adding phase_end_steps input to define the phases separately
	  from definition of the min timesteps in each phase.
	* StopLogic had a section that multiplies the min timesteps by 1.2 if max reaches above
	  the success threshold, which pushes it up to the phase 0 boundary.  I removed this
	  logic and increased the phase boundary a bit in case I want to bring that logic back.

1/1/23:

	* All trials failed badly, with max reward going steadily down from 10 to -2 at about
	  600k steps then staying there (70% did this, the others had bigger drops). The max
	  starting distance gradually drops over the first 800k steps, which explains most of
	  this behavior.  Mean rewards were all over between -40 and -28, but generally climbed
	  well until 300k steps; some continued climbing (or dropped then came back) until 700k
	  steps. Mins stayed clustered around -55.  All trials stopped at 1M steps.

* Next run
	* Added a tuning choice for NN size of [128, 50], which was used on 12/17 for successful
	  solo driving.
	* Changed noise schedule to end (magnitude 0.1) at 1.6M instead of 4M, which is what gave
	  success for solo driving.
	* Enhanced the reset() max_distance calc to allow an initial period with the full track
	  length before ramping it down.  Initially set it at 200k steps before ramping begins.
	* One trial had a max reward that stayed above 0, and continued past 1.2M steps, then
	  suddenly tanked. So it never reached phase 1, which begins at 1.3M. This trial used
	  a NN of [512, 64].
	* All other trials stopped at 800k steps, showing mean reward growth through 200k then
	  gradually decreasing; max rewards stayed at 10 until 200k then gradually headed to
	  negative.  Peak mean reward was as high as -20.

* Next run
	* Changed randomized start distance calc so that it doesn't begin to ramp down until
	  700k steps, then takes until 1M steps to completely disappear.  Hoping this will give
	  the reward enough time to become positive before the situations become more difficult.

1/2/23:

	* All trials stopped at 800k steps. Their mean rewards were climbing until 700k, then
	  headed down, while max rewards stayed at 10 until that point, then went down very
	  quickly. There were two groups, with the first group achieving a peak between -20 and
	  -10 (mean), and the second group running distinctly lower and peaking just below -20.
	  In the first group, the best 3 trials were all [128, 50] networks and had noise
	  magnitude between 0.48 and 0.65. The second group all had noise magnitude > 0.70.

* Next run (c8a85, 14 trials)
	* Giving more chance of choosing a [128, 50] NN.
	* Redefining phase 0 to be just random starting point, and extending it to 1M steps.
	  Then phase 1 will be gradually ramping down the starting distance, to 1.6M steps and
	  the phase will end at 1.7M steps. Then phase 2 will be neighbor vehicles for remainder
	  of 4M steps.
	* In the first 3 trials (all [128, 50]) there is some indication of a major step change
	  at 1.6M steps, causing the trail to go very badly and terminate at 1.7M.
	* Had to kill this run in middle of trial 5 due to shutdown for vacation.  Results are
	  still available - run the tensorboard server again.

 **	* AI: considerations for next runs:
		* the reward curve gradually flattens out as it progresses. I wonder if this is
		  due to the LR reduction.  Maybe leave LR higher for longer to see if it helps.
		  LR tapers from 1e-4 to 1e-5 over first 800k steps, where the reward slope is
		  pretty steep. Then it stays at 1e-5 between 800k and 1.6M, where the reward
		  slope is nearly flat and even starts to go a little negative. Then LR starts
		  to drop again to 1e-6 over the next several million steps.
		* consider extending the noise out longer

2/15/23:

* Next run
	* Set seed to a constant value, since varying it just creates an additional variable that
	  may be clouding the story of what works & doesn't.
	* Stretched out the LR schedule, per above, so it doesn't hit 1e-5 until 1.6M steps.
	  After that it is the same as before.
	* All trials looked similar to recent previous runs, where the reward curve slope gradually
	  decreases after a few hundred k steps, so that it is pretty flat after 1M, and nothing
	  reaches above mean reward of -10.

2/19/23:
* Compared code between current branch (3-neighbors) and master, which was last committed around
  mid-Dec with successful solo driving performance.  There is not a significant change that
  should affect agent capability.
* Next run
	* Modified LR schedule so it holds 1e-4 all the way until 1.6M steps instead of gradually
	  decreasing it, in the hopes of encouraging the same growth rate in mean rewards
	  throughout the trial.
	* No good results. Two runs peaked above mean reward of 0, one reaching ~5. But their
	  inference was terrible. All trials fell off to mean reward ~-40 between 1.6M and 1.7M
	  steps, so none saw any beneficial training with neighbor vehicles.

===== UPGRADED TO RAY 2.2.0 =====

2/22/23

Created a new conda env cda0, based upon the new env rllib22, which includes Ray 2.2.0 and several
other updated packages.  I then installed pygame 2.1.3 (with pip).  Tried running the cda0_tune
program as a system test.
	* It died immediately, complaining that tensorflow_probability package doesn't exist, so
	  I installed it (v0.19.0).  The tune program now runs, as does the plots program using
	  tensorboard.
	* Confirmed the gpu tester is working properly also.
	* The inference program didn't work at first because the config params code needed to be
	  rewritten.  Once I did that it works fine.
	* Updated the rllib22 env to include tensorflow_probability as well.
**	* Updated the tune program to use the new config objects of Ray 2.2. Best reference for this
	  is to look at the source code under ray/rllib/algorithms/algorithm_config.py (the base
	  class), and also in the derived classes algorithms/pg/pg_config.py and
	  algorithms/ppo/ppo.py (derived from pg).

2/24/23

***	Created new Git branch tune-checkpointing off of 3-neighbors, specifically to build a raw
	checkpointing capability to support better curriculum learning.

3/1/23

===== UPGRADED TO RAY 2.3.0 =====

* Created a new conda env cda0 based on a copy of the new rllib23 env, but with upgrade to torchvision 0.14.1.
  I then installed pygame 2.2.0 with pip to complete the env.
	* The GPU tester program passes in this new env.
	* Ran the existing cda0_tune program, which did a lot more environment model checking and gave me lots
	  of error messages, indicating a need to make it compatible with the gym 0.26 API (now called
	  gymnasium).
	* Reinstalled gym 0.26.1 in addition to gymnasium 0.26, to cover some legacy RLlib code that still uses it.
	* Modified some of my code to use more strong typing in environment reset method. The tune program now
	  runs fine.

*** Notes from my research in how to use checkpoints and do some real curriculum learning.
	* rllib/policy/torch/torch_policy_v2 has methods get_weights() -> ModelWeights and set_weights(weights)
	* Same class also has export_model(export_dir), and will create a checkpoint named model.pt, also
	  import_model_from_h5(import_file), and loads it into self.model apparently. But a model's state
	  includes network weights, optimizer state dict, exploration state, timestep info, etc. I only
	  want to keep the weights, I think.
	* self.model is created with internal method _init_model_and_dist_class(), which allows use
	  of an overridden method self.make_model(). It appears I can subclass TorchPolicyV2 (or without V2)
	  and create my own make_model() method, as this one doesn't do anything.
	* If make_model() isn't overridden, then it calls its own get_model_V2() method with several params.
	* Air Checkpoint class manages everything about checkpoints, which contain states of both algorithms
	  and policies.
	* Instead, use AlgorithmConfig class's checkpointing() method to define behavior with param
	  export_native_model_files = True, then after training use Algorithm.get_policy().
	* Consider using the ray/rllib/models/ModelV2 class (actually, classes derived from it).
	* Tune checkpoints include full experiment data, so are specified with the experiment top level dir
	  (e.g. in my recent use it is ~/ray_results/cda0).  Of course, if there are multiple runs in here it
	  could get confused.  Starting with a clean dir, I made a run, saved a couple checkpoints, then killed
	  it.  I then started a new run specifying the experiment checkpoint.  It said the checkpoint was
	  successfully loaded, but the run started over with iteration 1 and a different selection of HPs than
	  was used on the previous run.  Not sure it really worked.

	* Got some help on Ray discuss from @justinvyu on 2/28. I plan to implement his 3 suggestions:
		* Suggestion 1 - use a single env that swaps tasks (training levels or lessons)
			* Set up the API compliance and tested it.
			* Updated the environment code to handle levels 0-3 with two additional reserved for
			  future. Did short test with level 0.

		* Suggestion 2 - add PBT scheduler with this new environment.
			* Modified cda0_tune to incorporate code analogous to the examples provided.
				* Initial run - first perturbation it modified gamma outside of the range I
				  expected. I gave the schedule a mutation range of loguniform(0.95, 0.9999), but
				  rather than sampling from there it just multiplied the previous value by 0.8.
				  It seems it sometimes chooses to resample from the config values given, and
				  sometimes chooses to multiply by either 0.8 or 1.2.
				* However, in one mutation, it multiplied gamma by 1.2 and ended up with gamma > 1,
				  which is illegal, but it continued to run.  If I can't find a way to limit this,
				  then gamma is not a good candidate for perturbation!
				* When I specified num_cpus_per_worker = 1 and num_rollout_workers = 1, it set up
				  8 rollout workers, each with its own cpu. So it evaluated the 24 trials 8 at a
				  time for 20 iters, then perturbed them all and started another round of 20 iters.
				* Changing the train_batch_size to 200 (according to what it should be) still resulted
				  in 8 cpus being used at a time.
				* Specifying num_cpus_per_worker = 0 doesn't work.
				* Specifying num_cpus_for_local_worker = 0 is acceptable. However, not it runs
				  12 trials in parallel.
				* Specifying num_rollout_workers = 13, train_batch_size = 2600, I see only one trial
				  running at a time.
				* Posted a question about resource allocation to Ray Discuss.

		* Suggestion 3 - implement your own CurriculumScheduler

3/2/23

* Updated my environment model (SimpleRoadwayRamp) to provide multiple learning levels (tasks), per the Ray 
  curriculum training guidance (see notes above, including advice from a Ray Discuss contributor).  Also updated
  the wrapper class, tuner program and stop logic to accommodate these changes.
	* It doesn't really work well, since StopLogic can't talk to env, and number of iterations and timesteps
	  is only approx, since each trial arrives at them independently; also PBT scheduler resets the iteration
	  counter (the one that is available to env) after eacn perturbation cycle.  I think I'd rather do a single
	  learning level per run and manually adjust it.

3/3/23

* Updated the tuning program to implement population based training (PBT), per various Ray documentation and hints
  from my Ray Discuss thread.
	* Made a new run with 24 trials using HPs and tuning HPs that I think have a good chance for success over
	  all 4 defined learning levels.
		* The mean rewards all moved up asymptotically to 0 but never crossed it, and the min rewards never
		  exceeded -45 for any significant amount. The network structure was [128, 50], so maybe just not big
		  enough? I killed it at 220 iterations (~600k steps).
		* PBT had built a consensus that hidden layer activation should be relu and output activation should
		  be tanh. It didn't seem to settle on a good gamma; in fact, several trials still have gamma > 1.
		  Noise values (stddev) are still all over the place, as well, as are LR values.

* New run without gamma variation, but with network [256, 256] and perturb_int increased to 30 iters. 16 trials.
	* Found a defect in the set_task() method that killed any trial attempting to promote to the next level.
	  PBT with synch = True then waits forever for the errored trials to catch up to the next perturb point
	  and they never do, so the whole run hangs.

* New run with a fix for set_task hang, and the following additional tuning & mutating params:
	* tuning/mutating kl_coeff (default is 0.2) try uniform(0.24, 0.8). Set clip_actions = True and tune
	  clip_param = uniform(0.1, 0.3). Tune entropy_coeff = uniform(0.0, 0.01) or tune/mutate in uniform(0.0, 0.008).
	* Many times a thread hit the success threshold and got promoted to level 1, and a few times a trial got
	  promoted to level2, but the mean rewards never stayed in the success region. Worse, the min rewards continue
	  to stay down in the -40 region.  Until I can get rid of that, I don't feel like success is real.
	* The stop logic is not working, because internally the iteration count keeps getting reset at each
	  perturbation cycle. Also, the steps_this_phase continues to increase through phase changes in all trials.
	  It appears that steps_at_phase_begin is never getting updated. Finally, the passing of the env model to
	  it does not work. While it seems to store it okay, actually referencing it in the __call__() method
	  doesn't happen, so there seems to be no way for the env and the stopper to communicate.

3/9/23 - I've spent the last few days trying to get the Tuner program to start up and load weights from the NN in a
	 previously saved checkpoint.  A slow conversation with Ray staff on the Ray Discuss is working through it.

3/10/23

* Working to test checkpoint injection capability.  First step is to get a checkpoint worthy of ingestion, one where
  the agent can perform straight driving pretty well. This will enable me to easily verify whether that checkpoint
  was loaded correctly or not.
	* Notes above indicate that straight driving success was achieved on 12/17/22. I looked at the saved checkpoint
	  from that run and found it to be a single file (checkpoint-600), not a full file structure like I have been
	  recently studying and experiencing. Therefore, I don't believe it will work.
	* Pulled code from that Git checkpoint, which happens to be the current head of the master branch, and tried
	  to run it to regenerate the checkpoint. Since I have upgraded the system to Ray 2.3, this code no longer
	  runs. I went back to my tune-checkpoints branch head, then modified the tune program to meet the same config
	  that was used on 12/17. I also modified the env model so that it won't promote levels, but stays at level 0,
	  since my stopping criteria are currently hosed up. 
	* Made a new run with this setup (05f87), but with new HP search params & logic (using PBT), hoping to achieve
	  similar level of training of the agent. It found a solution with mean rewards > 8 pretty quickly (1-2 hr).
	  This checkpoint is now saved in ~/project/cda0/test/level0-pt.
* I tried running the tune program with the above checkpoint hard-coded in the CdaCallbacks class to load by the
  Algorithm code.  It now hangs if the from_checkpoint() line is there (not even when it's executed, just if it is
  uncommented). The hang is in the TrialRunner.step() method for some reason.
* Also, the inference program no longer works. It displays a mysterious error in the Ray code, after I have done
  nothing to it except set the NN size paramater to match what's in the checkpoint. It seems it can't handle the new
  checkpoint format (?)
	* Commenting out the class variable and all its uses doesn't fix the hanging. But changing the checkpoint
	  string to an invalid checkpoint (e.g. the parent dir) allows it to continue running, only with an exception
	  raised about the invalid checkpoint.

3/11/23

	* Forced python not to buffer stdout, and found that problem is in Algorithm.from_state() where it tries to
	  create the new PPO algorithm object. The PPO class doesn't have a constructor, so I assume it just runs the
	  parent's, which is Algorithm.
	* Additional print stmts throughout RLlib code shows that there is a call to ray.wait() that never comes back,
	  made from FaultTolerantActorManager.__fetch_result(), which results from a call to WorkerSet.add_workers()
	  from Algorithm.setup(). Posting the gory details to Ray Discuss thread for insight.

3/12/23

	* Got the inference program working. The call signature for env.reset() needed to be updated.

3/15/23

* Tentatively got checkpoint restore working for use in Tuner. Restoring individual Policy checkpoints rather than the
  whole Algorithm checkpoint.
	* In the process of of figuring this out (besides help from Ray staff), I added print stmts to:
		rllib.evaluation.rollout_worker.py
		rllib.utils.torch_utils.py
		rllib.utils.actor_manager.py
		_private.worker.py
		rllib.utils.checkpoint.py
		rllib.evaluation.worker_set.py
		rllib.algorithms.algorithm.py
		tune.execution.trial_runner.py
		tune.tune.py	
		tune.trainable.trainable.py
		tune.impl.tuner_internal.py
****	* Verified that checkpoints are now working!

========================================
===== Curriculum training for cda0 =====
========================================

* Began training level 0 (experiment 7ea15)
	* I had adjusted the reward function to remove completion bonus and to reduce the magnitude of penalties for
	  crashing or stopping (e.g. crash into vehicle went from -50 to -5) in an attempt to narrow the range of
	  potential rewards and therefore keep the reward gradient more moderate. However, this didn't learn very
	  well, so I restored it almost to its original (Dec 22) logic; the exception was that completion reward is
	  now a flat 10 points (for level 0), instead of being scaled based on time to complete.

3/16/23

	* 13 of 15 trials terminated successfully (min reward > 5), which put the mean rewards all close to 10.
	  Most trials took no more than ~90k steps.
	* Inference tests revealed:
		* Trial 1 YES, with 4/4 runs getting reward > 4, and one > 9.
		* Trial 2 no good. Accel was pretty limited, and even wrong sign sometimes.
		* Trial 3 no good. Accel was not as much as some of the others, and scores < 5.
		* trial 6 is very small accels, and even wrong sign in some cases.
		* trial 9 YES, with 4/4 runs getting reward > 4, one at 9.8.
		* Trial 10 no good. Accel is virtually 0.
		* trial 14 no good. It has ~0 accel, and is happy to go way too fast or too slow.
		* All tested trials perform well on lane change - no maneuvers at all, and LC cmd close to 0.
	* Saved trials 1 & 9 in project dir (and checked them into Git).
***	* Considering level 0 complete.

* Ran training level 1 (experiment ef817) starting with level 0 checkpoint from trial 1.
	* Prior to running, I changed the reward structure to give higher weights to the high-speed and low-speed
	  penalties.
	* Only one trial, #13, passed the stop criterion of min reward > 5, and that after ~500k steps. All other trials
	  settled out with mean rewards in the 0-5 range and min rewards in -60 to -10. Inference on this performed
	  really well, accelerating smartly from low initial speeds, and 4/4 achieving episode rewards > 7.
***	* Considering level 1 complete.

* Level 2 is not needed, since its objective is addressed with the level 1 solution.  Moving on to level 3.

* Level 3 training begins, to teach driving on lane 2 and making the lane change to survive.
	* Starting run aaf00

3/17/23

	* Training didn't work well. After 280k steps, only one trial had a mean reward > -10. Three trials errored,
	  and the remainder paused forever waiting on them. So the setting for recovering workers doesn't help this
	  situation with PBT. It seems the only answer is to not force it to sync.
	* The 3 errors were from the Gaussian noise calculation (distribution.py) trying to work with tensors full
	  of `nan`. Feels like a Ray defect.
	* It seems that the agent isn't exploring lane changes enough to figure out the solution, so probably needs
	  more noise.
	* However, running inference on the best one (run 9 got mean reward ~-5), it handles the lane change well
	  (never ran off road in 4/4 runs starting in lane 2). However, it has some sloppy speed control, usually
	  preferring lower speeds. It consistently got rewards in the 7-8 range, however. Saving this run in the
	  ray_results dir in case I want to use it as a baseline. Its reward curves look promising, like it would
	  have probably reached the goal if it had been allowed to continue.

* Next run (41fcb) started from the aaf00 trial 9 checkpoint saved in the previous run.
	* Tweaked initial noise std dev to range [0.1, 0.5]. Also increased its duration from 200k to 250k steps.
	* Set the PBT scheduler's synch param to False.
	* Configured for 1 rollout worker, which increases the number of parallel trials from 2 to 4. I think this is
	  important for asynch PBT perturbations, so that each trial has more to look at before making its decision.
	* Reducing the num rollout workers, however, reduces the number of steps per iteration. Even though there are
	  more trials running in parallel, they are doing less work, so the total progress against the wall clock is
	  maybe no better than having more workers running fewer trials.
	* Several of the trials passed the stop criterion of min reward > 0, but I'm not happy with it. First, these
	  ended up with mean rewards < 5. Second, these conditions just suddenly appeared, in every trial, for a single
	  iteration, where the previous iteration had mins around -50. With small batch sizes, it's hard to believe that
	  the agent has suddenly figured it all out in a single iteration. I would rather see the trend gradually grow
	  to the threshold value.
	* Inference runs on a couple of the "successful" trials showed performance no better than the starting
	  baseline.

* Next run (5501e) starting again from the aaf00 trial 9 checkpoint.
	* Increased num_rollout_workers from 1 to 4, and gave each 0.25 gpu.
	* Increased sgd_minibatch from 32 to 64 to help smooth out results. To accommodate, I increased rollout
	  fragment length from 200 to 256, and train_batch_size from 200 to 1024.

3/18/23

	* None of the trials got a mean reward > 0, but all ran to iteration limit of 800.
	* I suspect that the NN is just not large enough to handle the secondary reactions to its chosen actions.

* Next run (e43eb) starting from scratch and learning level 0.
	* Increased NN structure from [128, 50] to [256, 128]

3/19/23

	* All trials finished successfully, the longest taking ~420k steps. Trial 0 had both the mean and min rewards
	  climbing most gradually toward the end, so both were in good shape for a few iterations. I chose this one
	  to move forward, after confirming its inference performed well.

* Next run (eee95) started from the previous level 0 checkpoint (e43eb) and is training to level 2 with no HP changes.
	* First to complete was trial 10, after 210k steps. It worked well in inference, although it sometimes
	  commands negative accel (~-0.08) when speed is below the limit (26.8 m/s) with significant distance remaining.

3/20/23

	* Trial 3 reached a high mean reward of 8.4 with a min of 6.7 at 847k steps, which met the success criteria,
	  but everything else stayed level in the 4-6 range after ~400k time steps.
	* Inference on trial 10 looks very good. On one run its total score was only 5.15, due to a low starting speed.
	  In recovering from that it piled up lots of large low-speed penalties (~0.5/step initially). It was fairly
	  aggressive on the accel, but could have done more. It maxed out around 0.48; not sure why it didn't try up
	  to 1.0. This is disappointing, but not terrible. It also probably explains why nobody else was able to get
	  averages (or mins) any higher than they did, so I need to lower my success criteria or ease up on the
	  penalty calcs.
	* Inference on trial 3 is pretty good also, but it has a little more tendency to lower speeds and lower accels
	  than 10, so leaving it behind.

* Next run (e629c) at level 3
	* Changed max iterations from 800 to 500 and tweaked the success/failure criteria a bit to work with relaxed
	  speed penalties.
	* Changed reward function to relax the stiff speed penalties that were applied for levels 1 & 2. I figure the
	  agent has learned speed control at those levels, and now just needs a gentle reminder, as was in level 0.
	* Changed stopping criteria to use evaluation metrics instead of training metrics; also changed evaluation
	  config to go at every 10 iters (was 20) and in parallel to training runs to keep it from slowing things down.
	  The tag prefix to be used for evaluation metrics seems to be "sampler_results/", not "evaluation/"
	* None of the trials even came close to succeeding, which is strange, since it learned level 3 pretty well
	  last time around.
**	* I noticed that the CdaCallbacks.on_algorithm_init() is being called (and thus re-loading the baseline
	  checkpoint) every time a trial passes its perturb interval. This seems to be having the effect of erasing all
	  of the learning that has happened in the past perturb cycle. I initiated a question in Ray Discuss.

* Next run (2d9c3) at level 3, using normal random search on HPs (not PBT).
	* Starting from checkpoint of level 2 yesterday (eee95); no other changes from the previous run.
	* Log confirms that it is only restoring from the checkpoint once per trial.
	* Several trials looked promising until around 200k to 300k steps, then reward dramatically dropped or leveled
	  off. The noise schedule stops at 250k steps.

* Next run (02121) at level 3, repeated the above, but extended the noise degradation out to 500k steps.
	* This was a little better. One trial reached mean reward ~7, one reached ~3 and another reached ~0. There was
	  a bit of a drop-off after 500k, but less severe than before.
	* Of these three, the HP ranges used were
		* clip_param in [0.12, 0.25] from original range [0.1, 0.3]
		* entropy_coeff	in [0.00060, 0.0027] from original range [0, 0.008]
		* stddev noise in [0.19, 0.24] from original range [0.1, 0.5]
		* kl_coeff in [0.36, 0.71] from original range [0.35, 0.8]
		* LR in [3.2e-5, 14e-5] from original range [1e-6, 1e-3]

3/21/23

* Next run () at level 3, but this time starting from scratch (no baseline checkpoint)
	* Tightened some of the HP search space around the effective ranges above.
	* Since it is starting from scratch, it needs to learn everything, so extended max iterations from 500 to 1000
	  and noise schedule from 500k to 1.2M time steps. Also revised noise final scale from 0.1 to 0.2 to keep it
	  more involved.
	* NOT using curriculum logic, just letting it run completely at level 3.
	* No winners, but a couple trials hit mean reward ~ +5, so this is better. A few looked like they were still on
	  upward trajectories when terminated at 1000 iterations, but mins are all at -20 or less.

* Next run (7c62d) at level 3, starting from the level 2 checkpoint eee95. Otherwise, everything is same as previous
  run.

3/22/23

	* Trial 9 was fairly successful, ending with a gradual climb to mean reward of 7.4 and min reward of 2.8. This
	  did not meet the stopping criteria, but could be acceptable anyway. HPs for this run were:
		* clip_param = 0.264
		* entropy_coeff = 0.00287
		* stddev = 0.229
		* kl_coeff = 0.734
		* lr = 7.6e-5
	  Two others reached above zero (trial 10
	  mean reward maxed out at 5.4 and trial 7 reached mean reward of 2.5, but the best min for both of these was
	  ~ -40).
	* Inference on trial 9 shows pretty good performance, but it likes speeds a little on the low side (20-25 m/s).
	  Rewards are consistently in the 8s. It seems it could use some remedial speed control training.
	* Stored the trial 9 checkpoint for future reference.

* Run 7a976 starting from the level 3 checkpoint (7c62d), but running level 2 to help refresh its memory of good
  speed control. Limited to 200 iterations.  All other HPs the same as previous run.
	* All trials performed similarly, maxing out at ~ -20 for mean reward. Even the max rewards were often < 0, and
	  for 2 trials they stayed locked in at -40 and -28.  How can this be, if it has already 
	  succeeded at level 2? I hope it's due to the noise.  Need to try running without any noise.

* Run 6bdc1 attempting to repeat the above run, but this time with exploration = False.
	* None of the trials performed well. The best ones had mean rewards <-20.
	* I'm done trying to re-train the level 3 agent at this time.  Moving on.

* Run 7de26 - learning level 4, starting from the level 3 checkpoint (7c62d).
	* Expanded the HP search range a little bit, noise & LR, in particular.

3/23/23

	* Two trials had a promising trajectory, 12 and 13, with mean rewards climbing towards 0, but everything else
	  stayed at -40.  The HPs for these two defined the ranges
		clip_param in [0.10, 0.29]
		entropy_coeff in [0.0015, 0.0030]
		stddev in [0.22, 0.27]
		kl_coeff in [0.54, 0.57]
		lr in [1.6e-6, 9.9e-5]

* Run f3974 is a repeat of the previous run, trying level 4, but with the HPs tuning in the ranges listed above. Also
  extended the max duration out to 1200 iterations.
	* Most trials maintained mean reward at -40. Four climbed above, with the best trial (#2) reaching > -5, while
	  its max reward was near 10 for a long time.
	* Trial 2 used 
		clip_param = 0.19
		entropy_coeff = 0.0022
		stddev = 0.27
		kl_coeff = 0.53
		lr = 1.6e-6
	* Inference runs on trial 2 showed decent performance in all lanes, except that it tends to go way slower than
	  speed limit. When in lane 1 it starte behind the neighbor cars, so they always pull away from it. The lane 2
	  starts are interesting, but it's hard to tell if it is trying to avoid a collision with speed changes, or if
	  that is just a natural effect of the random starting state. I need to give user more control of that starting
	  situation.
	* Storing trial 2 for future reference, as the only example of level 4 action I have at this point. Probably
	  not demo-worthy, but it wouldn't be terrible.

* Run 64981 training level 4 starting from scratch. I opened up the HP search speace some more, and allowed it 2000
  iterations. I also turned on PBT scheduler, with first perturbation at 70 iterations, then every 50 afterward.

3/24/23

	* Four trials succeeded! Three of those four not only beat the mean reward target but also the min target
	  simultaneously.  These are:
		* Trial 1, rmean = 7.5, rmin = 5.7 at ~780k steps
		* Trial 14, rmean = 5.1, rmin = -40 at ~1.1M steps
		* Trial 2, rmean = 7.7, rmin = 6.5 at ~1.3M steps
		* Trial 9, rmean = 7.3, rmin = 5.4 at ~1.6M steps
	* All of the trials reached rmax ~10 before 500k steps and stayed there for the remainder. Most of the rmins
	  remained in the -50 to -40 range.
	* 10 of the trials errored out before completing. Every one was due to the common nan error in the Gaussian
	  noise generator. Maybe this has to do with interaction with PBT? I never saw it before using PBT.
	* Inference on trial 2 shows good performance from all 3 lanes, except that it likes the lower speeds (as low
	  as 12 m/s, when speed limit is 29 m/s). During ramp merges, it definitely slows down to avoid the neighbors
	  and then makes 2 lane changes to get on their far side, then accelerates smartly (69%) to get back to a
	  reasonable speed.
	* Training could be improved with better positioning of the ramp vehicle & its init speed. Also need a higher
	  low speed penalty.

* Run c9ded at level 4 with PPO and PBT from scratch. Same HP search space as previous run.
	* Changed reward penalties for low-speed to be a bit more severe.
	* Lengthened lane 2 so that the ramp vehicle can start with the neighbors and arrive at the merge approx
	  together while both are still close to the speed limit (don't want it to try to change its ramp speed
	  greatly).
	* Added method initialize_ramp_vehicle_speed() to try to force the ramp vehicle to hit merge point at particular
	  positions relative to the chain of neighbors.
	* After 200k steps all of the trials showed max reward < -25! This continued till > 500k steps, by which point
	  all mean rewards were coalescing ~ -32. So I killed it in favor of starting a duplicate run.

* Run c8429 is an almost duplicate of the previous run conditions - level 4 starting from scratch.
	* I increased the lower end of the noise range from 0.18 to 0.25, since all of the previous trials had converged
	  on 0.19, and I don't think that is enough to help them find their way around the neighbor vehicles well. This
	  time, the noise stddev values are all on the high side, with several > 0.5, and the smallest at 0.31.

3/25/23

	* Every trial quickly reached a state with both rmin and rmax = -40, so there was no learning going on.

* Played with turning on the curriculum_fn again to let it begin the training in level 0, then switch over to level 4
  just before the first perturb cycle. I got the timing of that down, but the problem is that PBT starts each perturb
  cycle by creating brand-new algorithm objects and environments, instead of reusing previous ones. So the environment
  loses all history and thinks it is starting over at level 0 again.
	* Created a new CallbackSingleton class to act as a singleton object that all instances of environment and
	  algorithms and CdaCallbacks can share info through. I made this a separate module that is first instantiated
	  by main (the tuner program) to ensure that it consistently lives through all creation cycles. Problem is
	  apparently each worker is a complete new process that re-instantiates _everything_ for itself, and doesn't
	  know about any of the other workers.
	* I think what I really need is this singleton to live in the local_worker and have all other workers somehow
	  do an IPC to share that info. Not sure how to do that.
	* In the meantime, I'll experiment with running a single local worker. No good. Even the local worker apparently
	  runs multiple trial threads, and recreates everythiing as well.
	* Designing a method that uses file system to track singleton-like behavior. This will work as long as all Ray
	  activity is on a single node, which is fine for now. It is the PerturbationController class. Tests show that
	  it works very well and doesn't seem to add a big execution time burden.

* Run 81302 started at level 4, using the level 3 checkpoint 7c62d as the baseline.
	* Early concerns in that the first 2 trials have rmax = -40 for the first 70 iterations.

3/26/23

	* Indeed, all trials scored a consistent -40, which is consistent with running off the road.
	* Log shows that after first perturb cycle it was correctly not restoring the checkpoint, and stayed in level 4.

* Run 6bee2 for level 4, but starting at level 0 from the level 3 checkpoint.
	* Enabled the curriculum_fn again, so that it will promote from level 0 to level 4 after 18k steps.
	* Modified env reset() so that in level 4 it only chooses vehicle position in lane 2 at relative loc 3 or 4, which
	  may help it avoid some collisions.
	* It transitioned from level 0 to 4 at 18k steps total (not per trial), at which point the rmax went from 10 to
	  -40 and stayed there. I killed it after 6 trials.
	* However, the evaluation plot shows that it continues to get rmax near 10 all the way to 70k steps.
	* I am starting to suspect this level 3 baseline checkpoint is no good.

* Run 9669f starting from scratch at level 0, then getting promoted to level 4 after 60k time steps.
	* Changed the promotion code to increase the number of steps.
	* Training progressed really well, with all trials showing improved rewards through ~350k steps, then they leveled
	  off with rmean between -10 and 0 and a few short peaks up to the +2 to +5 region, but nothing sustained. And the
	  rmins remain near -50.

3/27/23

	* ~1M steps (920 iters) a few trials started showing some serious rewards, in the 6-7 range. Although the stopping
	  condition for level 4 is rmean > 7, they are continuing. I started the program with cmd line arg of level 0, and
	  it internally promoted to level 4. Therefore, the main program is using the level 0 stoopping criteria, which are
	  stiffer.
	* After 1000 iters (~1.3M steps) all but one of the trials had met one of the level 0 stopping criteria, which was
	  impressive. However,...
	* Inference runs on trials 12 and 13 were very disappointing, as they did illegal lane changes consistently very
	  early in every episode. I did some deep investigation into the Ray code to ensure that it was truly inferencing,
	  with the model in eval mode and not applying any noise. The LC cmd (action[1]) coming out of the NN was well over
	  1, and relied on clipping to get it into the acceptable action space. I chose these two agents as they had good
	  values of both rmean and rmin in the training data _and_ rmin > 8 in the evaluation plot. I don't know how an
	  agent can perform so well in evaluation mode and fail so misrably during inference.

* run 8fc2d starting from level 0 (running level 1) checkpoint then getting promoted to level 4 after 60k time steps.
	* Adjusted PBT params to perturb only every 100 iterations.
	* Starting with the level 0 checkpoint is hoped to kick-start learniing a little sooner, and also to preserve more
	  respect for speed controls.
	* Adjusted reward structure so that crash, off road and stopping all get penalized at -10 points (was -50, -40 and
	  -20, respectively). I saw lots of inference episodes that chose to stop, which gave it a better score than running
	  off road or crashing. Also, the smaller magnitude may help it determine reward gradients in these regions.
	* No good. One trial started with rmax spiking above 0 several times in the first 60k steps, but then it stayed at
	  -10, along with all the other trials. PBT therefore had nothing to choose from and it never recovered.

* Run ed794 started from the level 0 checkpoint but running level 0 training to allow the variable start locations near the
  finish line. Otherwise, nothing changed from previous run.
	* No good. I discovered that at each perturb cycle the env is getting regenerated at difficulty level 0, even thoubh
	  there was a promotion to 4 previously. During perturbation new envs get generated, and they read the config dict,
	  which reflects the original cmd line arg. This may explain why the long run last night took so long to converge;
	  it just wasn't obvious that it was resetting.

* Run ? started from the level 0 checkpoint but running level 0 training, as before. Promoting to level 4 after 60k steps.
	* Fixed logic in curriculum_fn to ensure that it forces level 4 upon restart after a perturb cycle has occurred
	  (uses the new PerturbationControl).

3/28/23

	* The pre-training didn't really help. After 60k steps the rmax fell to -10 for all trials. However, rmax gradually
	  rose to +10 over 1M steps, showing some promise. But the rmean stayed near -10 until the ~800k steps or so, when
	  some of the trials showed fluctuations up to -7.

=========================================
===== Began using the SAC algorithm =====
=========================================

* Had been using PPO up to this point, but it seems to be time for some more drastic changes to continue progressing.
* Starting with mostly the default configs until I get a feel for it.
* Cannot use any of the previously saved checkpoints. Also cannot use the existing checkpoint handling code in the
  CdaCallbacks class (I will upgrade it in the future to do this).
* Updated the storage scheme for checkpoints, since I'm getting so many of them with too many attributes.

* Run 32efc training level 0 starting from scratch (no usable checkpoints) with PBT perturbing every 100 iterations.

3/29/23

	* All trials terminated at 1100 iterations, having not achieved the success criteria. But all got pretty close,
	  in the range of 5-7 for rmean.  Just needed a little more time.
	* SAC seems to move through the iterations a lot faster than PPO.
	* Saved the best checkpoint (trial 10) under SAC L0.

* Run 9eb73 attempting to finish what the previous run started, so using its checkpoint, but still training level 0.
	* Checkpoint restore using the existing CdaCallbacks code worked fine, with the sample printing turned off.
	* Interesting that the first several iterations produce training rewards that look like the agent was never
	  trained before.
	* Training ended similarly to previous, with rmean values in the 6-8 range, but there were a couple spikes
	  into the low 9s (just shy of the stopping condition). Even so, the best trial (7) when run in inference
	  was really bad, changing lanes frequently, and thus making illegal ones often. Hard to understand, given
	  the rmean values, and even evaluation runs looked similarly good.

* Run ? starting from scratch at level 0, then promoting to level 3
	* Adjusted noise level downward.
	* Turned on curriculum auto promotion from phase 0 to phase 3 after 80k steps.
	* No good. rmean actually started dropping after the switch to level 3. A couple trials spiked to 
	  rmean > 9.5 and terminated successfully. But inference on one of them was terrible.

*** Switched to DISCRETE ACTION SPACE with 23 accel values.
	* Turns out SAC can't handle a MultiDiscrete action space, so I also had to switch back to PPO.
	* Did this on new branch discrete-actions off of alt-training.

=======================
===== Back to PPO =====
=======================

* Run b948a started from scratch training just level 0 using discrete action space.

3/31/23

	* All trials eventually met the stop criteria, every one hitting the rmin threshold (6.0) before hitting
	  the rmean threshold (9.5); the final rmin values were all in [8.1, 8.9].
	* I did inference on trials 4 and 14 (best rmean) and found that #4 performs really well, so keeping it
	  for future reference.

* Run 1aa52 starting from the level 0 checkpoint just saved and training for level 2 using PPO and discrete actions.

4/1/23

	* 15 of the trials errored out, nearly at the same time, between 1M and 1.2M steps. The final trial ran to
	  the limit of 2000 iterations and stopped. None scored more than rmean ~ 6.5, and none had rmin > -10.
	* Sampling the error files, it seems they all just died of unknown segfaults.

* Run bd6cc starting from the level 0 checkpoint and training for level 2 (PPO with discrete actions).
	* Reduced sgd_minibatch_size from 64 to 16 to try getting a little more variation in rewards.
	* Reduced the number of trials from 16 to 8 since they all have been closely packed togehter in the reward
	  plots.
	* None of the trials saw rmax > -10 prior to the first perturb cycle, so there seems to be no way it will
	  learn anything.

4/2/23

* Lost most of a day due to power interruption, which somehow screws up the GPU driver.

* Run d9e0d starting from the level 0 checkpoint and training for level 2 (PPO with discrete actions).
	* Increased sgd_minibatch_size from 16 to 128.
	* Using 10 trials.

4/3/23

	* All trials behaved very similarly, gradually improving to an asymptotic approach to rmean ~6 and
	  rmax ~9 and rmin in the range -10 to -2 after 2000 iterations (2.7M steps). The rmax was still slowly
	  increasing, so more time may have improved results slightly.
	* The best result was trial 6, with ending rmean = 5.9 and rmax = 9.0. It used 
		clip_param = 0.056
		entropy_coeff = 0.0030
		kl_coeff = 0.74
		lr = 2.56e-5
	* In inference, trial 6 performed really well, so it's a keeper, and the new baseline.
	* I discovered that PBT was perturbing 3 variables that are for SAC, so weren't even being used.
	  No wonder it took so long for this solution to perform well.

* Run 0ed7f starting from the level 2 checkpoint (d9e0d) and training for level 3 (PPO with discrete actions).
	* Fixed the PBT perturb params to use the PPO variables.

4/4/23

	* Got quite good results on 9 of 10 trials in < 1M steps. All of them beat the rmin criterion of 6
	  while achieving rmean in the range of 8.
	* Trial 6 performed the best, with rmean = 8.02, rmin = 6.00 and
		clip_param = 0.27
		entropy_coeff = 0.00071
		kl_coeff = 0.80
		lr = 1.61e-4
	* Inference worked very well, so this is the new baseline.
	* Realized that I need a reward deadband in the speed dimension. with none it is almost never choosing an
	  accel cmd = 0, rather it constantly tries to tweak the behavior, which is unnecessary.

* Run 319ad starting from the above level 3 checkpoint and training for level 4 (PPO with discrete actions).
	* Adjusted reward function to allow a small deadband of speed just below the speed limit.
	* Adjusted PBT schedule to perturb every 80 iterations instead of every 100.
	* No good. All trials roughly consolidated around rmean of -4. Inference on one was pitiful on lane 2 (it
	  consistently made illegal lane change in first time step).

* Run 2ce8a repeated previous run, but with 2000 iteration limit. Maybe different prng draws will find something.
	* Worked well! A couple trials found a high rmax before the first perturbation, and the others soon
	  followed. After 1200 iterations all trials were still active, but tightly bunched around rmean ~8.2,
	  with many iterations showing rmin ~+8.
	* Trial 3 was the only one that showed a consistently high rmin of 7+ for an extended time (over 140k
	  steps), so I chose this one to keep.
	* Inference on this trial showed good performance on all lanes. However, it revealed that my assumptions
	  in the reset() method were poor for setting the initial speed when in lane 2. Instead of staying
	  close to that speed on average, it immediately accelerated to get up to the speed limit (training
	  taught it this will maximize reward, crash notwithstanding). 

* Run 65cf4 repeated previous run, but with initial conditions improved. Starting with same level 3 checkpoint and
  training for level 4 (PPO with discrete actions).
	* I enhanced the initialize_ramp_vehicle_speed() to assume max accel then stay at speed limit as the
	  nominially desired trajectory (without neighbors). This is the trajectory it will need to alter to
	  avoid a collision, so the generated initial speed will force it to learn to accept some penalties
	  in the ramp speed profile in order to avoid a crash.

4/5/23

	* All trials ran to iteration limit. 
	* The best trial at the end was #0, with rmean = 3.5. Second best was #9 at rmean = 2.9. The trial with
	  largest rmean towards the end was #2 with rmean = 7.0 on a brief spike for one iteration around 1.98M
	  steps, maybe ~1700 iterations. However, no checkpoint was stored near there.
	* Inference on trial 0 works great on lanes 0 & 1. It works pretty well on lane 2, but it seems to have
	  learned a simple behavior of always slowing down (to 24-25 m/s), then changing lanes, hoping the
	  neighbors have already passed by then. This works well if it is timed for the end of the congestion,
	  but usually results in a crash if it is timed for the beginning. Not a terrible policy, but it doesn't
	  seem to be looking at the neighbor data dynamically. This could be because:
		* the NN is too small to accommodate that level of thinking, or
		* the observation space makes it too difficult to understand what is going on.
	* Inference on trial 9 performed similarly, but a few more crashes.

*** The above results are good enough for my objectives for level 4.  Need to start moving on.

=============================
===== For fun larger NN =====
=============================

* just for grins...training level 4 from scratch (no checkpoint) using PPO and discrete action space, but
  also with a larger NN ([400, 256]) to see if that is a factor in how it learned in the previous run. I'm doing this
  to kill time while I take care of other life priorities.
	* The runs in this section are not on the clock, as they are just playing and general education.
	* Run e6215
		* Since it won't have much chance to learn plan speed control on lane 2, I changed the reset() method to
		  hold back the neighbor vehicles 10% of the time when in level 4.
		* Initial try had all trials running at rmean = -10 since it never learned how to steer straight. Turned
		  on the curriculum_fn to do 80k steps at level 0 then promote to level 4 and ran again.

4/6/23

		* It didn't do much. Most trials held rmean ~-6 and then spiked up to +10 for a single iteration, triggering
		  the stop criterion without really learning enough. So I...
	* Run 1e8ce repeated the above for-fun run, but with a new stopper that forces rmean to exceed the success threshold
	  (on average) for at least 5 iterations before declaring done. Also changed the speed penalty to be symmetrical
	  instead of heavily weighting the high-speed performance.
		* Didn't work. All trials stayed at rmean = -10 except for a handful of spikes > 0, but no real learning.
	* Run c5c7e repeated the above for-fun run, but only does the curriculum transition from level 0 to level 3 so that
	  it can properly learn the basic speed control and lane change tasks. Then it will be introduced to neighbor
	  vehicles, which will now have somewhat randomized starting distance and speed.

4/8/23

	* Run 86ab5 training just level 0 since it seems the automatic curriculum is not working so well (it isn't learning
	  level 3 stuff, and each perturb cycle spikes the rmean to 10 because new envs are created at level 0).
	* Run ??? training level 3 starting at the above level 0 checkpoint.
		* Didn't work. All trials leveled out ~ rmean = -4.
	* Run 88c1e training level 2, starting from the prev level 0 checkpoint.
		* Worked well. All trials leveled out with rmean ~6.
	* Run 2e382 continued the training for level 3, based on the above level 2 checkpoint (trial 9).
		* No good. All trials had volatile rmean in the range of -7 to -3.
		* Inference on a couple trials showed good performance on lanes 0 & 1, but couldn't get strted on lane 2,
		  as if it had never been trained on lane 2. I added print stmts to confirm that most of the level 3
		  episodes start on lane 2 during training runs, and other starting values are reasonable.
		* Ran a second training run with same settings to confirm that prng wasn't being wonky. No good again.

4/9/23

	* Run df4c6 rerun of level 3 training starting from level 2 checkpoint above.
		* Realized the prev run included lots of changes, so backed out everything in the env model (using the
		  simple_highway_with_ramp file from the master branch), but still using the larger NN and new stopper.
		* Still no good. rmean leveled out quickly in the range -7 to -3 for all trials. Same story as above.
		* Tentatively concluding that the [400, 256] NN structure isn't going to work here.
	* Run bce56 going back to the NN of [256, 128], but with the new logic for speed penalty. Training level 3
	  starting from the level 2 checkpoint made on 4/3 (d9e0d). This run should be contrasted against run 0ed7f
	  done on 4/3.
		* Also leveled out noisily between -7 and -3. It appears it's the baseline whose training is bad.

4/10/23

	* Run 5f786 starting from scratch to train level 0 on a [256, 128] NN using the new, symmetrical speed penalty.
		* All trials moved to rmean ~ +10 quickly. I kept trial01 for future use.
	* Run c23d4 training level 2 from previous level 0 checkpoint usiing symmetrical speed penalty.
		* Worked well. Trial 8 was best, and being saved.
	* Run 9e59e training level 3 from the previous level 2 checkpoint using symmetrical speed penalty.
		* Performed very well. All trials reached rmean ~8 by 600k steps. Lots of frequent spikes of rmin up
		  to the 0 to +4 range in the last half of the run (up to 1.3M steps).
		* Trial 7 was best, and worked very well in inference on all 3 lanes. Saved for reference.

4/11/23

	* Run 75472 training level 4 from previous level 3 checkpoint using symmetrical speed penalty. This one also
	  includes the new variability in neighbor vehicle initial positions and speeds, which should give the agent
	  a more generalized understanding of how to interpret their motion and avoid them.
		* Learning was mediocre. After ~700k steps all trials maintained a noisy rmean between -2 and +2,
		  but never seemed to learn more. Run stopped after 1000 iterations.
		* The best trial was 5, which I saved for future reference.
		* Inference shows that it learned a simple policy (in lane 2) of "slow down to 26 m/s and hope that
		  the neighbors go past, then change lanes". If it comes in near the front of the pack it doesn't
		  try to speed up and get in front. It accepts the crash.

**	* Now to re-try the larger NN to see if that resolves the problems noted above. I now have the symmetrical
	  speed penalties and the variable initial conditions for the neighbor vehicles. But need to start training
	  the whole shebang from scratch.
	* Run cc914 training level 0 from scratch with [400, 256] NN, symmetrical speed penalty.
		* Two trials terminated early, after 300 iterations, achieving nearly perfect scores. All the others
		  performed well by the end of 800 iterations, with rmean ~8. Saving the best, trial 0.
	* Run 9b3a9 training level 2 starting from previous level 0 checkpoint with NN [400, 256] and sym speed pen.

4/12/23

		* Worked well. Saved trial 7 as reference. All trials reached rmean ~8.
	* Run 2e253 training level 3 starting from previous level 2 checkpoint with NN [400, 256] and sym speed pen.
		* No good. After 400k steps all trials settled into a noisy, flat rmean between -6 and -4, with
		  rmax almost 10 all the way, but rmin never above -10.
		* This feels just like the results on 4/8 and 4/9 with the larger NN. And none of the perturbed
		  HPs converged to a dominant value, each one is still all over the place at 800 iterations.
	* Run cc955 duplicating the above run just to give it more random initialized trials to see if one catches.
		* No good - identical results as previous. It would appear that the larger NN is incapable of
		  learning at this level, as NN size is the only difference from the success in run 9e59e on 4/10.

4/13/23

		* Extended this run by starting from one of its best late checkpoints (trial 5 at iteration 740),
		  which had rmean = -3.5. Letting it run for an additional 800 iterations (run 878ef) on level 3.
			* This run generated the same results, so I have to conclude that this NN structure just
			  isn't going to work for level 3. I don't understand why making a network larger would
			  compromise its ability to learn. I would expect it to learn better, even though it may
			  take longer.

==================================
===== New observation vector =====
==================================

* Based on the experiments of past few days, and some introspection, it seems the agent is having a hard time
  understanding the neighbors' states and what it means for it. So I modified the observation space to provide it
  a more intuitive picture. 
	* Working in the new new-obs Git branch, off of the adv-level4 branch from develop.
	* Removed neighbor distance remaining (not being used)
	* Replaced neighbor X (distance downtrack), which could be confusing since each lane has a different 
	  origin), with delta distance downtrack (the neighbor's distance ahead of the ego vehicle)
	* Replaced neighbor speed with delta speed above that of the ego vehicle. This will magnify small diffs
	  so that they have a bigger impact.
	* Eliminated the ego vehicle lane ID as a variable input to the NN (always set to 0) so that it can't learn
	  specialized behavior just on this value, and forces it to look at the other inputs more.
	* I considered changing the roadway code so that everything uses the same X origin, but decided it would
	  be riskier and take more time. Using the adjusted downtrack dist is pretty safe, if a little slower
	  and a little more confusing for the reader.

* Run 31b3b training level 0 with the new obs; back to NN [256, 128] and using symmetrical speed penalties
	* Worked very well, with all trials passing rmean > 9.5 and several having rmin > 8 for several iterations.
	  (note that all the changes don't impact this level, except for agent not seeing its own lane ID.)

* Run 6428b training level 2 from the previously saved level 0 checkpoint.
	* Good. All trials had rmean > 2 before 500k steps, then gradually climbed to between 6 & 8, with rmin
	  often shooting above -10. Best trial was 2, which I kept for reference. Inference on it is fine.

* Run 1579d training level 3 from the previously saved level 2 checkpoint.
	* No good. All iters quickly settled into rmean between -6 and -3 up to 800 iterations.

4/18/23

* Run 3cf40 training level 3 again, this time starting from level 0 checkpoint.

4/19/23

	* No good. As before, rmean in -6 to -3 range, through 1000 iterations, although rmax ~10 for all.

* run 3ec58 training level 3, starting directly from scratch.
	* Considered changing the randomized start dist for level 0 to apply, but with PBT it risks always
	  randomizing no matter how mature the training has become.
	* Nothing doing. All rmax = -10.

* run 6ad8f training level 3 starting from scratch, same as previous. However...
	* Change env code to randomize initial agent position for first 60k steps in levels 0-3; also reduced
	  initial lane ID emphasis on lane 2 (for level 3) from 70% to only 50%.

4/20/23

	* Worked well. rmeans leveled out a 2 to 4 until 800k steps, then they jumped up again until ~1200k
	  steps, where they leveled off between 7 and 9.5. Best was trial 2, ending early at rmean = 9.5.
	* It appears it is experiencing some problems starting from a pre-trained checkpoint (catastrophic
	  forgetting?).

* Run f024c training level 4 starting from the previous level 3 checkpoint (6ad8f) with NN [256, 128], PPO
  and discrete actions with the new observation space.
	* After 1400 iterations, all trials were stuck in a run with rmean between -3 and -1. Inference on
	  trial 2 at this point showed that it had learned to avoid a crash by going full throttle all the
	  time; when in lane 2 it just outran the neighbors, and its reward was ~-0.5, much better than that
	  for a crash. It also hit full throttle in the other lanes as well, so not seeing opportunities.

* Run ba89e training level 4 from scratch (extending the randomized positions for first 60k steps to this level).
	* Adjusted random lane assignment for level 4 from 70% on lane to to 50% on lane 2.
	* Also adjusted LR range top end from 1e-3 to 3e-4
	* Adjusted entropy_coeff range top end from 0.008 to 0.01

4/21/23

	* Moderately successful. For all trials, rmean very gradually made a noisy climb to between +1 and +4
	  at 1500 iterations. It may still have momentum to keep going.
	* Best trial at end was 0, with rmean = 3.9, rmin = -10. Inference proved very bad, with illegal lane
	  changes everywhere.
	* I noticed that the code change made to randomize starting position was not what I had intended, and
	  produced substandard training conditions relative to neighbor positioning.

* Run c4c9f training level 4 from scratch as a re-do of the previous run.
	* Fixed code for level 4 randomized initial location to use the actual level 4 logic for the other
	  initializations also.

4/22/23

	* Training for 1500 iterations saw noisy rmean end in the range +3 to +7, but appearing to be on a
	  slow, upward trend. I suspect another 1500 iterations may be able to improve it.
	* Trial 0 had the highest rmean toward the end (~7 at checkpoint 1350). Inference on it worked well
	  for lanes 0 & 1, but most runs on lane 2 had illegal lane change in first couple steps.
	* Trial 9 performed similarly.

* Run fabc8 training level 4 as a direct continuation of the above run, starting with its trial00 checkpoint
  1350. No other changes made.

4/23/23

	* Good. Didn't get much learning improvement over the previous run. All trials leveled out quickly
	  in a range of rmean in +5 to +8 (after 200k steps) and rmin stayed at -10 except for a few spikes.
	* Best trial was #0. Inference shows that it performs well in lanes 0 & 1, and often does well in
	  lane 2. But at least 50% of runs on lane 2 have illegal lane change on the first step, always to
	  the right. Almost all of these occur when initial speed is really low (< 7 m/s). For some reason
	  these mostly seem to occur when specifying relative positions 2 or 3.
	* Found a code defect in initialize_ramp_vehicle_speed() solving the quadratic equation, which could
	  explain some unexpected starting speeds, and maybe the lack of training at the very low speeds.
	  Also found that reset() is initializing ego_x differently in inference than in training, such that
	  inference is trying values that were never trained.
	* Realized, during this fix, that many times there is no solution for initial speed, because there is
	  too much time available, even if initial speed is 0. In order to meet the target time in many cases
	  the accel would have to be less than max allowed. This is why a trained agent always tries to get
	  in front of the neighbor platoon - it has very little experience getting behind it.

* Run 7f4e2 training level 4 starting from checkpoint L4-c4c9f, as a re-run of the previous run.
	* Applied fixes to both defects noted above.
	* Increased the possible downtrack starting range for neighbor vehicles, given the propensity for
	  their arrival time to be too long, thus encouraging the agent to learn to get behind them sometimes.
	* Training performed about the same as before, as did inference runs. Not worth keeping.
	* Found & fixed a defect (after training) in initializing ego_x when in lane 1; it sometimes put the
	  vehicle at a negative start distance.

===============================
===== SAC algorithm again =====
===============================

* Converted the environment code to use a flattened, 1D Discrete action space, since SAC can't handle a
  MultiDiscrete. The action space involves 51 actions, made by slimming down the number of acceleration choices
  from 23 to 17, and repeating them for each of the 3 lane change commands.
	* Reenabled the commented-out SAC model config variables in the tune program. Also changed up the HPs
	  used in the PBT scheduler.

* Run f4e8f training level 4 from scratch. All HPs remain unchanged from the PPO runs, except those that are
  algorithm-specific.

4/24/23

	* Terrible. rmax bounced rapidly between -10 and +10, but rmean never got above -10 for any trial.

* Run eb83b training level 4 from scratch. Adding search ranges to more HPs (tau, initial_alpha, n_step).
	* Similar performance. I killed it early.
	* It appears that each 100 iterations the variable start distance gets re-instated (using a step
	  counter that is being reset by the perturb cycle), then after the next 60k steps, the rmax drops to
	  -10 for the remainer of that 100 iters.

* Run f576c training level 4 from scratch, but this time went back to PPO while still using the new 1D action
  space. This is to verify that the action space is not causing problems.
	* Also added a perturb_ctrl action to reset() to ensure that once the first perturb cycle occurs there
	  will be no more randomized initial positions.

=========================================================
===== Restructuring with advice from Kevin Albarado =====
=========================================================

4/29/23

* I got some mentoring time from Kevin, who made several suggestions on how to change the approach.
	1 Use PPO or SAC - I will use PPO for now, since it has been working well for me.
	2 Use continuous action space, and discretize lane change action like I did originally.
	3 Change the action space to reflect the agent's goals, not the process of getting there. Instead of
	  accel and lane change, use speed and lane ID. Not sure how to generalize the lane ID part, but for
	  now, worth trying to see how well it works.
	4 Change the obs space to get closer to the raster-type approach I have been considering already, so
	  that the agent will look at spatial cells near it to figure out what is going on in the immediate
	  vicinity.
	5 Be sure to penalize sudden, large changes in any action command (this is currently in place).
	6 Keep the NN small, and go a little deeper (try 3 layers).

* Merged the git branch adv-level4 into develop, which now reflects the best of the above work in PPO.
  Created new branch "new-obs-actions" off of develop for this next phase of work.

* Run 72102 training level 4 from scratch, with suggestions #1 and #6 implemented (PPO).
	* NN structure is now [256, 256, 128].
	* Changed penalty for crash (with another vehicle) from -10 to -20 so it is worse than runnin off road.

5/2/23

	* After ~500k steps all trials settled into a noisy steady state where rmean fluctuates between
	  +1 and +5 and rmin fluctuates between -20 and -10.
	* Inference on best trial shows most runs on lane 2 drive off road in the first time step.

* Code mods for uniform coordinate system - replacing the concept of measuring a vehicle's location relative
  to its current lane's start point, since each lane may start at a different X value. Going forward, all
  vehicle locations will be represented as X coordinates, regarless of which lane it is in. 
	* Updated Roadway and Lane classes to support this concpet change.
	* Changed all the rest of the environment code to convert from DDT to X coordinates.
	* Ran a bit with debugging set to 1 and saw nothing crazy in the log.
	* Ran a training job with same settings as previous one, to demonstrate that it performs the same.
	  Results were pretty much like the previous run, so I'm comfortable moving forward.

5/4/23

	* Modified the action space from [accel cmd, LC cmd] to [desired speed, desired lane ID]. This
	  satisfies suggestions #2 and #3.

5/7/23

	* Modified the obs space so that it includes a grid of zones surrounding the ego vehicle, rather
	  than looking at data on specific neighbor and specific roadway geometry. This satisfies suggestions
	  #4 and #5.

5/9/23

	* Misc testing & fixing defects. Still have a problem with obs[6] sometimes being out of bounds.
	  I think I need to redefine the obs space only in terms of the scaled values, not the raw - need
	  to experiment with this.

5/11/23
11:52 - 12:44 = 0:52
21:17 - 21:47 = 0:30
22:00 - 22:20 = 0:20

	* Created new branch, scaled-space, from new-obs-actions to play with scaling the obs space.
	  Found that I can't do much with the derived class because it needs to overwrite the parent's member
	  observation_space variable. Aborted this idea.
	* Completed testing and began a new run...

* Run c6e1e first training run with the new observations & action space, using PPO.
	* Did not perform very well. rmean topped out ~4 for some trials, then dropped a bit.

5/12/23
11:32 - 12:54 = 1:22

	* Inference showed lane 0 works great, lane 1 starts often crashes into neighbors at the beginning.
	  I believe this is restart() putting it too close to the back of the platoon and it doesn't have a
	  chance to adjust. Lane 2 starts seem to always want to do an immediate lane change (to lane 1), so
	  I never saw a run go past the first time step.
	* Looked into updating the inference program for the new lane 2 schematic concept. Decided it's best
	  to revise comments, variable names, & concepts throughout to support a true map frame (reflecting
	  real geometry), a parametric frame (how the NN views it, with lanes parallel), and a display frame.
	  This will take some time to draw out, so I ran inference several times just for the calculations,
	  accepting that the graphics are bogus.

AI: redo coordinate transforms, then verify latest results with new inference program.
	- get_vehicle_dist_downtrack() should be removed (used by inference)

5/16/23
03:09 - 04:02 = 0:53

5/17/23
11:27 - 13:40 = 2:13

5/18/23
09:40 - 10:00 = 0:20
11:30 - 





Near-term considerations:
- implement level 5 with random lanes & crash avoidance neighbors
- study graph ML
- study path planning to replace near-sighted agent
- switch to raster map


*** Variables under manual control:
	* batch size
	* noise schedule
	* replay buffer HPs
	* PBT HPs
	* training session duration, stopping criteria, checkpointing and curriculum structure
	** learning algorithm
	** NN structure
	** action space design
	** observation space design
	** reward structure

AI: track down where Trainable/Algorithm object is instantiated and its setup() method is called, since this happens at
    the beginning of each trial, and at each perturbation of each trial in PBT.
AI: change reward structure so that gradients aren't so steep on failure.  Maybe total episode range should be [-1, 1]?
