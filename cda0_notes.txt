Notes on the cda0 project
=========================

10/3/22:
* Spent approx 3 hr setting up & beginning the coding.

10/4/22:
* Time spent:  
  17:00 - 18:44 = 1:44
  21:21 - 22:34 = 1:13
* Decided to go forward with the somewhat awkward and non-extensible approach of modeling observations to include states on
  exactly 3 neighbor vehicles.  For a future version I will replace that with a more general approach that looks at the
  roadway itself (analogous to an Atari agent viewing screen pixels rather than tracking a number of alien ships).

10/5/22:
* Time 20:45 - 22:40 = 1:55

10/6/22:
* Time
  10:33 - 12:39 = 2:06
  21:53 - 23:03 = 1:10

10/7/22:
* Time
  21:24 - 22:42 = 1:18
  23:01 - 00:30 = 1:29

10/8/22:
* Time
  17:04 - 18:25 = 1:21
  19:03 - 19:27 = 0:24

10/9/22:
* Time 19:54 - 22:53, int 13 = 2:46
* Completed unit testing of all env logic except reward.
* Built initial reward logic.

10/10/22:
* time 10:48 - 11:15 = 0:27

10/11/22:
* Time
  21:08 - 23:15 = 2:07
  23:44 - 00:15 = 0:31

10/12/22:
* Time
  08:14 - 09:40 = 1:26
  14:12 - 14:57 = 0:45
  15:27 - 15:56 = 0:29
* Fixed all unit test and run-time errors. Ready to begin training.

10/13/22:
* Time 22:08 - 01:00 = 2:52
* Still having occasional problems with observation_space violations

10/14/22:
* Time 23:08 - 01:59 = 2:51
* Created conda env "cda0" as a copy of rllib2 to make sure any project-specific changes aren't applied to the base env.

10/15/22:
* Time
  11:59 - 12:34 = 0:35
  22:44 - 00:20 = 1:36
* SUCCESS - first taste! got several trials to drive lane 0 beginning to end, forcing no steering.
  Best tuning trial was 35915_00003; saving checkpoint 181.
* Ran new training job (7559b), with results under the cda0-l0-free dir, that starts agent in lane 0 but allows it to change
  lanes.  Most successful trial was 75599b_00005; using checkpoint 126.  PPO lr = 2.8e-5, batch = 512.
* TODO: Need an inference program to run these successful models and capture their trajectories for viewing.

10/16/22:
* Time 21:05 - 23:09, int 20 = 1:44
* New tuning job (17669) under cda0-l01-free dir. This one randomly initializes episodes with the ego vehicle in either lane
  0 or 1, but not 2.  The neighbor vehicles still do not move. Two solutions got pretty close (00003 and 00002), but none
  scored higher than low 1.8s for the mean reward.
* Installed pygame (with pip) into the conda env "cda0" to experiment with making a graphical display of the simulation.
  Played with an example program enough to quickly understand how to do some basic graphics needed for my sim.

10/17/22:
* Time
  14:16 - 14:48 = 0:32
  17:10 - 18:36 = 1:26
  22:32 - 23:21 = 0:49
* Fixed a problem in the env wrapper that was locking the ego lane to 0; also changed reward shape some, so all previous
  training needs to be thrown away.
* Training again on random lane start (0 or 1 only) with no neighbor vehicles moving; trial ID = c2d96.
	* Trial 0003 ran for a long time, but reward gradually increased the whole time, maxing out above 4!  Not sure how
	  this is possible. LR = 0.00013, batch = 512.  It has learned to shy away from speeds near the max, since it gets
	  punished for large accels there; also shy of speeds near zero, for same reason, but this is less of a fear.
	  Result is that accel oscillates between +3 and -3 m/s^2, with speeds going between 4% and 75% of max range.
	  So avg speed of the vehicle is really small, and it collects more reward points for staying on track.
	* Trial 0007 was the only other one that succeeded, with a max reward around 1.9. LR = 0.000161, batch = 256.
* Increased penalty for jerk; decreased existential time step reward; made a more differentiable shape for penalty where
  accel is threatening to take vehicle past either high or low speed limit, in order to minimize accel oscillations.
* Training again on random lane start (0 or 1); trial ID = 47fd7. None of the 18 trials converged.

10/18/22:
* Time 21:40 - 11:00 = 1:20
* Removed reward penalties for both the acceleration limits and trying to keep steering command near {-1, 0, 1}.
  Ran tune (ID 10baa). Died in middle due to computer crash.
* Compared code to that used on 10/16 (commit cf6f) to find why suddenly nothing is learning to drive straight.
  Only found two seemingly minor diffs in the reward structure (given that the new penalties are commented out).
  I changed those back to the way they were on 10/16 and ran a new tuning run (b24d4).

10/19/22:
* Time
  17:46 - 18:25 = 0:39
  21:54 - 23:03 = 1:10
* Last night's run produced 3 winners (out of 20 trials). This is with the "old" reward structure, so just a baseline to 
  prove it can be trained.
	* Trial 14 had mean reward = 1.87; LR = 5.54e-5, activation = relu, network = [300, 128, 64], batch = 2048. Solved
	  in 121 iters. Its min reward was slightly < 0, however, so not idea.
	* Trial 16 had mean reward = 1.88 with similar min; LR = 3.42e-5, activation = relu, network = [300, 128, 64], batch = 512.
	* Trial 19 had mean reward = 1.89 with similar min; LR = 2.11e-5, activation = relu, network = [256, 256], batch = 512.
	  Use checkpoint 163.
	* Reviewing these results, I realized the reward was broken for lane change penalty, so fixed it.
* Another run (f7b05) applied these changes.  Found several successful trials.
	* Trial 19 had mean reward = 2.02 with similar min; LR = 1.36e-5, activation = tanh, network = [300, 128, 64], batch = 256.
	  Use checkpoint 154. 
	* Trial 18 had mean reward = 1.89 with min ~1.3; LR = 2.44e-5, activation = tanh, network = [300, 128, 64], batch = 1024.
	* Trial 15 had mean reward = 1.83 with an unimpressive min; LR = 5.60e-5, activation = relu, network = [256, 256], batch = 512.
	  Use checkpoint 126.
	* Trial 4 had mean reward = 1.86 with min ~1.3; LR = 3.39e-5, activation = relu, network = [300, 128, 64], batch = 2048.
	  Use checkpoint 97.

10/20/22:
* Time
  01:12 - 01:33 = 0:21
  17:15 - 18:31 = 1:16
  19:54 - 10:00 = 2:06
* Added the LCC penalty back into the reward method.  Made tuning run 211ad, all with a 3-layer network.
	* Trial 6 mean reward = 2.01, LR = 1.04e-4, activation = tanh, batch = 1024
	* Trial 11 mean reward = 1.75, LR = 1.99e-4, activation = tanh, batch = 1024
	* Trial 14 mean reward = 1.75, LR = 1.37e-5, activation = tanh, batch = 128; min reward above +1.5
	* Trial 18 mean reward = 1.76, LR = 1.67e-5, activation = tanh, batch = 128, min reward above +1.6
* Run 211ad showed
	* LC cmd penalty worked really well at keeping the LC command very close to zero
	* Accel was also really close to zero on trial 6, but averaged slightly positive so that speed rose throughout, but it never
	  reached the speed limit.  Trial 6 got only 0.796 completion reward, but gathered lots of time step rewards throughout.
	* Trial 11 had big accel, but its total reward was ~1.8 vs 2.2 for trial 6. So avg time step reward of ~0.09 is too high.
	* There is a strong desire to drive in lane 0, whether the initial lane is 0 or 1; LC occurs immediately if needed.
	* Max completion reward is only ~0.83 for the fastest possible travel time.  Needs to be increased.
* Added scaling of the action_space in the wrapper class to keep the NN output accel limited to [-1, 1] (it was in [-3, 3]).
  Modified the reward shape a bit to better emphasize completing the course as fast as possible.
* New tuning run (b5410) to finalize work on lane change issues.

10/21/22 Fri:
* Time
  16:52 - 19:37, int 23 = 2:22
  20:12 - 21:12 = 1:00
* One run from yesterday (b4510) succeeded, which was trial 1. Mean reward = 2.11, LR = 4.99e-5, activation = tanh, batch = 1024.
  Actions were well behaved, as desired, but accels were all small and tended to slow the car down to get more time step rewards.
	* Changed reward limits from [-1, 1] to [-2, 2], since the completion reward was being greatly clipped.
	* Reduced jerk penalty mult from 0.01 to 0.005.
	* Reduced time step reward from 0.005 to 0.002 (it was contributing >0.5 to the total episode reward)
	* Added penalty for exceeding speed limit (and increased obs space upper limit for speed substantially to allow an excess).
* Run fe601 with these changes produced no successful trials.  After observing one slightly promising run, made these changes:
	* Increased gamma from 0.99 to 0.999.
	* Reduced LR range a bit.
	* Added a HP to choose model's post_fcnet_activation between relu & tanh (was formerly fixed at relu).
* Another run with the above changes had no success either.  So I removed the speed limit penalty and created run 25ee5.
* During training, I continued to write the graphics code, but in separate copies of the files: inference.py,
  simple_highway_with_ramp.py, using the suffix "_new" on each one, so it won't affect the ongoing training.

10/22/22 Sat:
* Time
  10:18 - 11:00 = 0:42
  11:34 - 12:55 = 1:21
  13:50 - 15:01 = 1:11
  17:40 - 18:03 = 0:23
  19:55 - 21:25 = 1:30
* Runs from last night (92d01) that look promising are 3 (LR 9.36e-5, tanh/tanh, batch 1024) and 12 (LR 3.50e-5, relu/tanh, batch 512).
	* Run 3 ran close to 0 speed. When it finished, it collected 0 completion reward because it took 1176 time steps!
* Added penalty for slow speeds (normalized speed < 0.5), slightly increased penalty for jerk and slightly reduced penalty for 
  lane change command.
* Increased number of most_recent iterations to evaluate for stopping, and didn't stop if max reward is close to success threshold.
* Finished the graphics code for the initial roadway display and integrated it.
* Realized a MAJOR PROBLEM I have had: the training was always starting in lane 0. Also, since the vehicle initial conditions set by
  reset() were pretty limited (speed and location), it was often never seeing experiences for downtrack locations or for high speeds.
  Therefore, I modified reset() to randomize these, and the initial lane ID, over the full range of possible experiences.  It
  is not clear to me how to get Ray Tune to pass in random values for each episode (when reset() is called), so for now I'll
  depend on reset() to handle it.  I've added a "training" config option that opens up these random ranges; if it is False or
  undefined, the initial conditions will be as they were before.

10/23/22 Sun:
* Time
  15:11 - 15:30 = 0:19
  20:16 - 21:03 = 0:47
* Finally got one that seems to have learned: 10965 trial 10, used LR 5.80e-5, network [200, 100, 20] and output activation = tanh.
  However, this also did not perform well.  Accelerations are all over the place, and LC commands are as well.  Several inference
  runs failed to complete even half of the track.
* Added remaining code to the Graphics class in the inference program to do rudimentary display of ego vehicle driving on the track.
* Started a new training run using the DDPG algo instead of PPO (which I had been using thus far).  It seemed to produce some
  good results quickly, but the rewards never grew enough.  Started playing with the network structures more.
	* Some of the iterations are showing min rewards in the -200 to -300 range - how is this possible? Apparently, due to lots
	  of accumulated low-speed penalties.

10/24/22:
* time
  13:34 - 15:07 = 1:33
  19:57 - 22:31, int 15 = 2:19
* DDPG run from last night found some success!  Run 41f25, trial 3 used actor network of [256, 128] and LR = 9.6e-6, with critic
  network of [100, 16] and LR = 3.8e-4.  Inference run in lane 0 stayed there with small lane chg cmds the whole way, and gradually
  accelerated to max speed with no jerk penalties anywhere!  Running checkpoint 500 (after which the mean reward dropped a bit).
  Full run captured total episode reward of 1.54 taking 85 time steps.  Another inference run started with a low speed (0.16 scaled)
  and performed similarly, but, because of the low speed penalties at beginning, its total episode reward was only 0.34.
	* Trial 14 from that run also performed really well in inference, using actor network of [100, 16] and LR = 6.6e-6, and
	  critic network of [128, 32] and LR = 9.6e-4.
* Completed testing the graphics update method.  Had to modify the env wrapper to get access to both the scaled and uscaled obs.
* Added upper speed limit penalty and ran new training runs with DDPG, but not getting success. Max rewards tend to settle around 1.3
  while min rewards settle around -10 to -20, with means in the -5 to -10 range.  Maybe this settling is because noise gets removed
  too early?
* Started another run with:
	* Reduced upper speed limit penalty (0.2 at 1.2*speed limit)
	* Much larger noise decay schedule (from 90k timesteps to 900k), plus random noise for 20k timesteps.
	* Longer trials, up to 900 iters.

10/25/22:
* Time
  19:00 - 19:58 = 0:58
  21:45 - 23:22 = 1:37
* Last night's DDPG run did not succeed either.  From the four best ones, I see that the actor performed best with the largest
  network ([400, 100]), implying that maybe bigger would be better.  Also, it seemed to prefer LR ~2e-5.  The critic didn't seem
  to care as much about either of these params, so it is probably good with a smaller network.
* Created a new DDPG run, notably with a much larger replay buffer. Before, the default 50k experiences was used; now I am using
  1M experiences.  Also adjusted some HPs a bit.  Still no good.
* Run ac6fb: it seems that adding the upper speed limit penalty is causing it not to learn, so I removed that penalty and made no
  other changes for this run. This produced at least 2 successful trials!  It has therefore dawned on me that the problem is the
  magnitude of the penalties being imposed for approaching the speed limits.  They are way too big for a per-time step penalty,
  considering the other penalties are O(0.001) and these are O(0.1), especially when the offending situation is not possible to
  get out of in a single time step.  The negative reward piles up very quickly, discouraging any learning.
* Reduced the low & high speed penalties by about 2 orders of magnitude for new run fad55.

10/26/22:
* Time 20:04 - 21:12 = 1:08
* A few runs scored decent mean rewards (~0.9) very early, then they gradually dropped as the episodes went on. However, their
  early checkpoints perform pretty well.
	* It is willing to accept a penalty of ~0.003 for low speed and ln chg cmd (each). But it doesn't like a large jerk
	  penalty at all.  Seems to be willing to accept the low speed penalty in order to pick up more existence reward (0.005).
	* In another run that started much faster, it was willing to accept a high speed penalty of up to 0.014 for the entire
	  run of 100 steps, in order to pick up 1.14 points for completing the run fast.
* Changed rewards so that
	* Reduced existence reward to 0.003 (was 0.005)
	* jerk penalty maxes out at 0.006 (was much higher)
	* low speed penalty maxes out at 0.02 (was 0.01)
	* high speed penalty at 1.2x speed limit is 0.02 (was 0.01)
	* Reduced success threshold to 1.1 (was 1.2)

10/27/22:
* Time
  09:07 - 09:33 = 0:26
  11:53 - 12:25 = 0:32
  14:29 - 15:07 = 0:38
  16:29 - 16:51 = 0:22
  21:00 - 22:47 = 1:47
* Got a few runs whose mean reward peak came very early, and only hit ~0.2.  Common to accept lots of ongoing penalty for low
  speeds, but kept very gradual & small accels.
* Reduced jerk penalty.  Also removed the time step existence reward and reshaped the completion reward to drop off faster for
  slow traversal.
* Fixed defect in steps_since_reset counter initialization, which was causing success reward to be less than it should. Also
  set up new run with wider exploration of actor LR and noise params.
* Added condition to stop logic in case mean reward is low but max is above threshold; if mean is near the min than stop it.
* Turned off all lane change command penalties and ignore LC command coming into step() so it only has to learn about speed
  control.
  Still got no good results. This time each of the min, mean & max reward curves was almost flat, except for some noise, with
  the mean centered aroune 0.2, which is way below what it should be.  Running inference on one of the more successful trials,
  I see that it is on the gas all the way, maxing out high speed penalties, which accumulated to ~2x the completion reward.
  This doesn't make sense.  I therefore think it is learning that max speed is the best policy because each episode begins
  at a random downtrack location, so some of them are gathering close to 1.5 completion points for going very fast for only
  a few time steps.  Therefore, I changed the restart() method to always initialize training runs at the very beginning of
  the track so it has to drive the entire track to get a completion reward.
	* Debugging statments reveal that incoming accel actions, even at the very beginning of a training trial, are
	  highly correlated throughout the episode - mostly in the 0.8 - 1.2 m/s^2 range! But it is completing episodes
	  regularly very early.  It quickly learns to max out accel (+3.0) and minimize number of time steps, and never gets
	  to explore what happens below the speed limit.
* Replaced the OU noise model with Gaussian using sigma = 1.0 m/s^2 at the beginning (gradually annealed).  I confirmed that
  it initially produces accels all over the place.  However, it still quickly learns to push the accel to the max throughout
  the episode.

10/28/22:
* Time
  19:28 - 21:41 int 15 = 1:58
* Walked through a few series of manually specified accelerations through episodes in the new environment loop tester.  It
  appears all the rewards make sense.  Further, running a full episode at speed just below the speed limit gives a much
  better episode reward than going full throttle all the way through.
* Changed the hidden layer activations from relu to tanh for both actor & critic.  No improvement.
* Reduced the lower limit of the actor LR range.  Started to see some hope at the low end.  Need to go down into the e-7 area.
* Turned down the Gaussian noise sigma from 1.0 to 0.1.  No noticeable effect on the reward plots.
	* Inference on the best one of these (episode reward max ~0.8) showed that it seems to be learning to reduce its
	  accelerations (held close to 0.2), but it still lets speeds max out, so maybe slowing down the LR more will allow
	  it to discover the sweet spot.
* Reduced both actor & critic LRs even more.
* TIMING: on laptop battery, one episode of 145 iterations on 1 worker/1 env took 2:51.
	  1 worker/4 env took 3:01 on battery (took 3:09 on AC power)

10/29/22:
* Time
  09:00 - 09:18 = 0:18
  11:29 - 12:08 = 0:39
  16:15 - 18:23 = 2:08
* Runs from last night with much lower LRs kept the max reward steady > 2, and even mean rewards stayed steady instead of
  dropping, but still < -1, since the mins never improved.  It does appear that gamma = 0.999 is important, and a more
  narrow range of LRs is the sweet spot.
* More tuning with tau.  Doesn't make a big difference. Nor does adding more Gaussian noise.
* Switched to TD3 algorithm, using the defaults suggested in the algo manual (they aren't provided to copy).
* Took a break to train the racecar project, which is a simplified version of this, only trying to drive straight down
  a lane as fast as possible, but while respecting the speed limit.

10/30/22:
* Time
  15:08 - 16:09 = 1:01
  16:48 - 17:53 = 1:05
  21:33 - 22:56 = 1:23
* Racecar toy taught me that it is important to keep the cumulative amount of possible penalties (over the episode) on the
  same order of magnitude as the completion reward.  I had had them about 2 orders of magnitude larger.  I also suspect that
  it will be beneficial to just let the system train a lot longer than I have, even though there hasn't been any clear
  progress in a few hundred iterations.
* Applying these lessons to the cda0 project brought some quick success for the limited case of just speed control with TD3.
	* Critic was [256, 32] for all trials.
	* Found that actor network of [128, 16] is definitely too small.
	* Best performers had actor of [256, 32].  They all learned quickly and reward curves were smooth.
	* Actor network of [512, 128, 32] struggled to learn, but some trials did well. Best ones were the lowest LRs
	  (1.2e-6 for actor and 4.7e-6 for critic)
	* I was unable to get checkpoints to load for inference engine - error about differing state dict param sets,
	  even though I verified the network structures were specified the same.
	* Max rewards were smaller than I had hoped (but I can't see exactly what's going on due to no inference).
* New training run using DDPG and using lane change control also (just lanes 0 & 1).  Changed reset() to randomize the
  vehicle's initial location anywhere along the lane instead of just at the beginning (it was learning to just come to a
  stop to avoid the perpetual negative rewards).
	* Five of the 15 trials appear to have succeeded! Run 8c03d using critic net [256,32]. Results are in the
	  cda0-l01-free dir.
	* Trial 0 had a long & jagged learning curve, but got there. Actor [256, 32], actor LR = 2.5e-5, critic LR = 3.1e-5,
	  tau = 0.005.
	* Trial 2 had actor [256, 32], actor LR = 8.2e-6, critic LR = 1.6e-5, tau = 0.005.
	* Trial 10 had actor [256, 32], actor LR = 7.8e-7, critic LR = 1.1e-6, tau = 0.001.
	* Trial 11 had the fastest learniing curve, with actor [256, 32], actor LR = 9.6e-7, critic LR = 7.7e-5,
	  tau = 0.005.  In inference it used modestly high accel all the time and ignored the speed penalty. Rewards ~0.94.
	* Trial 12 had actor [256, 32], actor LR = 4.7e-6, critic LR = 1.8e-5, tau = 0.001.
	* None of the trials with an actor net of [512, 64] was even close.
* Next run includes the following mods:
	* Increased success threshold from 1.0 to 1.1, which is not reachable by going full throttle all the time.
	* Increased minimum required iterations to 300 to ensure we have plenty of settling time.
	* Added penalty for LC cmd values near +/-0.5. 
	* Doubled the penalty for high speed violation (previously maxed out at 0.001). 
	* Results of this run (be9bc) showed that the [200, 20] actor network could achieve success, but not as easily.
	  Also, inference of a couple winners showed they still prefer full speed and a lower episode reward.
* Next run (23f2e) doubled the high speed penalty again.

10/31/22 Mon:
* Time
  16:18 - 17:00 = 1:42
  20:50 - 21:15 = 0:25
* Results of 23f2e run:
	* Several trials reached episode reward between 1.0 and 1.05 quickly, and mostly stayed there.
	* None reached the success threshold of 1.1.
	* Best trials were 4, 7, 9, 13.  Others that reached 1.0 but had some downward spikes: 1, 8, 12
	* It appears that probability of success favors the [256, 32] network over the [200, 20] actor; also the larger
	  tau (0.005) seems more favored.  Also, as expected, a ratio of actor LR / critic LR ~0.1 seems best.
	* Inference shows that these models still want to use a high accel and are very reluctant to change its value.
* New run with following changes (3b68a):
	* Trying larger 2-layer networks (the 200 last time wanted higher accels)
	* Eliminate the jerk penalty to encourage large changes in accel.
	* Double the high speed penalty (it was maxing at 0.0041).
	* Use OU noise to be more realistic in generating variations in accel.
* Results: successful trials have mean rewards plateauing ~0.8, despite max rewards consistently being > 1.4. 
  Inference run still accelerates to max speed and stays there.  Lane chg commands remain close to 0, however.
* New run with following changes
	* Tuning noise magnitude
	* Tuning with larger actor network (512 nodes in first layer)
	* Tuning with larger choice of critic network
	* Doubled the high speed penalty again to give max of 0.016

11/1/22 Tue:
* Time
  04:07 - 04:47 = 0:40
  11:46 - 12:20 = 0:34
  15:56 - 16:32 = 0:36
  19:01 - 20:03 = 1:02
* Analysis of prev run:
	* No cases were successful.  However, 3 of them quickly reached mean reward ~0.5 and stayed there.  Then
	  2 of them fairly quickly (~500k steps) reached mean reward ~0.2 then slowly climbed to reach 0.8 after
	  7M steps.  It appears they would keep going with further training.
	* The two promising ones had both networks at [512, 64], noise in use (for 3M steps), and a LR ratio of
	  actor/critic close to 0.1 with actor LR between 2e-7 and 5e-7.  In each case their min reward stayed around
	  -1 and max stayed at 1.48.
	* On inference, they both had learned that small positive accel is the answer, and kept the LC cmd quite
	  small as well.  They did not learn to maximize speed within the safe range, however.
	* The four that got almost as high results showed a similar LR ratio, and 3/4 had the larger critic network.
* Modified reward code to randomly cancel the episode completion bonus if high speed violation occurs; probability
  of cancellation is proportional to the amount of excess speed involved in that time step.
	* Similar to previous run, a couple cases gradually increased mean reward (max ~0.7).  These were 0, 2, 3
	  (for a while, then dropped off).  These all had actor network of [512, 64] and critic network of [768, 80]
	  and similar LRs: actor ~3e-7, critic ~2e-5.
	* Inference on two of them showed same pattern of sticking to very small, positive accelerations, regardless
	  of initial speed, and letting it run into the high speed violation.
* Modified reset() method to change initial position of the vehicle during training.  It had been allowed to start
  anywhere along the route, but I feel that is encouraging it to go for the big score and ignore speed limits, and
  therefore, not worry much about accel.  As iterations progress, the initial position will gradually be squeezed
  toward the beginning of the track, forcing it to train for longer episodes.  Also adjusted the probability of
  cancelling the completion bonus upward (to worst case 4%).
	* Most cases resulted in flat mean reward curves plateauing at ~-0.2, so no good.  Three reached positive
	  territory, however.
	* All 3 best trials used actor of [512, 64] at LR between 2e-7 and 7e-7, and critic of [768, 80] at LR
	  between 2e-5 and 5e-5.
	* The biggest peak mean reward (trial 14) was 0.3, but it eventually tailed off to < 0 (after 5M steps).
	* The worst performing of the 3 (trial 12) peaked ~0.1, then quickly dropped to < 0 after 2.5M steps.
	* Inference runs show that these didn't perform any better than the previous training run.  They learned
	  to keep accel small, but have no idea that speeding up to the speed limit is advantageous, or that
	  slowing down if above it is good.
	* I confirmed that actions coming into the step() method tend to cover the full range of possible values,
	  at least early in a training run.
* Modified reward code to add an accel bonus if recent (4 steps) avg speed > speed limit and avg accel over that
  period is < 0, and vice versa for speeds below speed limit (exceept for a deadband).  Bonus increases with
  larger speed difference from the limit and with larger acceleration magnitude.

11/2/22:
* Time
  10:37 - 10:59 = 0:22
  12:51 - 13:38 = 0:47
  19:51 - 20:32 = 0:41
* Analyzed run from last night:
	* Pretty similar performance as before - 3 runs reached mean reward > 0, maxing ~0.3. 
	* Inference still shows a desire for accels very close to 0 and a slow change to it.  However, one run
	  demonstrated a slight response to the high speed penalty, where speed got into that zone, then accels
	  turned negative and returned it below speed limit.  It took many time steps, however.
* Modified reward code to double the probability of eliminating the completion reward if high speed, and
  doubled the accel bonus value.  
	* Running inference on some very early checkpoints (2-20 iterations) shows that already the accels and
	  LC cmds are very small.  This makes me wonder if there is a scaling problem.
	* Adding print statement during training run shows that scaling is not a problem; all calcs seem proper.
	  It is just learning very quickly (in first 20 iters) to keep accels close to zero.  I now suspect
	  that this may be due to too much smoothing by training in large batches.  Also the small time step
	  may be having some smoothing impact.
* Next set of mods:
	* Tuning with much smaller batch sizes (down to 8)
	* Simplified accel bonus calcs to just be based on current time step, not history.
* Analyzed above run (0c04c):
	* All cases had a mean reward > 0, but none of them exceeded 0.8 despite each one having a max > 1.3.
	* Runs 0 & 1 peaked quickly then dropped rapidly.  Batch sizes were 1024 and 128, respectively.
	* Run 2 took the longest to peak (6M+ steps) but also had the lowest max (as low as 1.25 at 7M). Its
	  batch size = 128.  LR among the smallest & largest at 1.2e-7 for actor and 9.3e-5 for critic.
	* Run 3 also peaked fairly quickly and dropped off a lot. Batch = 128.
	* Run 4 was a slow mover but peaked nicely. Batch = 16, actor LR 1.7e-7, one of the lowest.
	* Run 7 was lowest mean peak (but highest max), and dropped away very quickly.  The only batch = 8.
	* Runs 2, 5, 4, 13 had the lowest actor LRs, and their critic LRs were widely different. They all
	  showed gradual peak then tail-off of mean, plus max started high (~1.55), dipped, then climbed again
	  until its end.  The dip was lower for those whose means took longer to peak.  Once means tail off,
	  the max climbs again.  These had batch = 128, 16, 1024, 1024.  It seems they may have continued to
	  improve with more time.
	* Inference performed similar to previous runs - accel very small and slow to change, but they are
	  starting to see the correct directions to move.
	* It doesn't appear that small batch size has a noticeable effect.  Best bet seems to be LR ~1e-7 for
	  actor and 5e-5 to 9e-5 for critic, then let it run a lot longer.
* New run:
	* Using new StopLong class for the stopper, which pretty much lets it run to max iterations unless
	  the max reward is a failure.  Also extended max iterations to 2000.
	* Magnified the noise.
	* Tightened the LR ranges, and moved the actor lower and critic higher.
	* Doubled reward bonus for correct accel action.

11/3/22:
* Time
  10:31 - 10:59 = 0:28
  18:00 - 19:17 = 1:17
  21:13 - ?
* Analysis of last night's run
	* Some runs got peak mean reward of close to 0.7, similar to previous. Most has max rewards >= 2.
	* Case 1 and 7 started very slowly, then gradually increased mean reward; the only two that didn't
	  drop off within the 2000 iterations.  Peak value of mean was ~0.4.  Also, their max values were on
	  the lower side (~1), then began to climb towards the end.  They are ripe for additional training. 
	  These both had batch = 8 and 2 of the lowest actor LRs (7e-8 and 9e-8), with the same critic LR
	  of 9.8e-5.
	* Case 6 is also interesting, as possibly the best performing of the others, with peak rewards
	  ~8M steps.
	* Inference results are similar to previous, however. Not satisfying.
* Mods for a new run:
	* Check for even smaller LRs for actor, larger for critic.  Also, throw in a couple really big ones.
	* Try batch size = 4.
    ***** Can config params get passed into the env object for each run?  Yes - the configs are passed in to
	  the init method, and it is called by each worker at the beginning of an iteration.  Any values
	  will be held constant throughout all episodes of that iteration (just like extrinsic items are,
	  such as LR).  This could be a means to schedule gradual changes in individual reward penalties or
	  bonuses, or even in environment dynamics, such as taking off training wheels (removing limits on
	  action outputs).  It could also be used to randomize some env constants to effectively provide
	  data augmentation (e.g. change friction coefficients, control response times, control biases, etc).
* Analysis of above run (e9151):
	* Case 2 looks maybe promising in late time steps - long, steady mean reward, probably the highest
	  mean and max reward near the 12.5M step mark.  Actor LR = 1e-5, critic LR = 9e-5, batch = 8.
	* Case 3 very similar to 2 but much less erratic reward plots.  Actor LR = 4e-8, critic LR = 9e-5,
	  batch = 16.
	* Case 5 no good but had an interesting, huge spike in both mean and max rewards at ~2M steps, then
	  terminated soon after.  Is this where noise ended?
	* None of the 3 cases with batch = 4 was able to hold a mean reward > 0.  At least one had what I
	  believe is a good LR ratio.
	* Inference runs (of case 2) show that it is definitely reacting to the high and low speed penalties
	  and/or accel rewards, but its reaction time is many seconds, not one or two time steps as I would
   **     like to see.  Is it possible that the unused historical accel values in the obs vector arc
	  contributing to that?  Hard to believe, but maybe.  No - experiment shows it is not.
   **   * Other things to try:  play with size of completion reward (even make it zero at all times) or
	  at least ratio of completion reward to penalty magnitudes (maybe the all need to be closer to 1?).
	  Try PPO or another algo to get more dynamic action response to changing conditions.
	  Try training with a slow-down zone in the road to force larger accels.

Took a few days off and played with the simpler Racecar project to get back to basics. This is in the
projects/cda0_copy dir.  An even simpler variant, Car, is in the projects/copy2 dir, and in Github under
repo name simple-car.  Used PPO as the training algo.

LESSONS LEARNED from simple-car:
* I was able to train a car to drive a straight lane without traffic, as fast as possible while obeying the
  speed limit.
* Keeping time step rewards large enough to present noticeable impacts on derivatives is important (values
  in O(1e-3) or O(1e-4) were not doing it). Final time step's completion reward was O(10) and incremental
  values were O(0.01) for 150 time step episodes.
* Shaping all rewards/penalties as smoothly differentiable seems to be important. Using quadratic function
  of distance from target value is better than piecewise linear.
* Start with really small NN structure.  I was getting good results with a [16, 4] FC network.
* HP tuning takes a LOT of time.  Most of them don't have much impact, but LR is very sensitive.
* May need to get several trials of ~same LR before finding one that works with initial weights distro.
* May need to let training run a long time so it can gradually converge after several chaotic dips in 
  performance.
* Important to get a good balance of noise, which probably needs to be extended throughout most of the
  trial.
* Be sure noise is explicitly turned off during inference runs!  Using Ray algorithms may bring in unseen
  config settings that turns it on by default (e.g. PPO's "explore" flag).
* Ray makes it very difficult to continue training from a Tuner checkpoint.  For insights, see
	* https://discuss.ray.io/t/save-and-reuse-checkpoints-in-ray-2-0-version/8169
	* https://discuss.ray.io/t/correct-way-of-using-tuner-restore/8247
	* https://discuss.ray.io/t/retraining-a-loaded-checkpoint-using-tuner-fit-with-different-config/7994/7

11/19/22:
* Time recorded directly on dashboard
* Merged the reward and tuning code from the simple Car code into the cda0 code and ran a tuning run there in
  attempt to duplicate the success of training a drive in a straight lane with no other traffic around.  The
  only difference is that the cda0 environment now involves all the other observation elements and a 2D action
  vector (although the 2nd element is not yet used).
	* Results: two of the first 4 trials achieved mean rewards > 9.  Running them in inference (with no
	  noise) starting in either lane 0 or 1 showed good performance.  However, it seemed to be a little
	  too afraid of jerk, and happier to go slowly and take a lower completion reward.
	* The one trial that had random_start_dist turned off never went anywhere, so it seems this is essential.
* Trying a new run with the low speed penalty turned on and a little smaller jerk penalty.
	* This was still really tame in terms of acceleration & jerk, so accepted some slower-than-desired
	  solutions, with very slow, smooth accels.
* New run with ligher jerk penalty again, by 10x more, and with 2.5x more low-speed penalty.
	* Trial 00006 performed beautifully! Smart accel at the beginning, and smoothly leveled off when speed
	  approached posted limit, then stayed there.  Exactly what I wanted.  This is saved as trial
	  PPO_SimpleHighwayRampWrapper_1656e_00006 under ray_results/cda0-l01-free, and against code committed
	  on 11/20, commit fe894d0.
*****
* Straight lane performance is now complete.  Time to move on.

11/20/22:
* Time recorded in dashboard
* Ran the cda0 tuning code with all 3 lanes as starting options, meaning in lane 2 the agent has to learn to change
  lanes in order to finish.  Results are now being recorded in ray_results/cda0.
	* Results were poor.  None of the 15 trials got a mean reward > -8 or so, although the max reward was
	  consistently ~ +16.
	* One trial aborted due to a Ray error, but was looking like it could possibly break out to higher ground
	  than the others.  Its LR = 4.5e-5.

12/4/22:
* Time recorded in dashboard
* Narrowed the LR range a bit and increased the noise magnitude (stddev) from 0.3 to 0.5 (Gaussian).

12/5/22:
* Results from yesterday's run:
	* Nothing got above mean reward of 0, but one trial came close, peaking twice around -8 before falling off.
	  Running inference on this one's peak checkpoint...
	* Starting in lane 0, it ran smoothly, but at a nearly constant, small positive accel, thus hitting top
	  speed eventually, and taking a penalty of 0.4 per time step.  Total score = 10.4, but it could have done
	  a lot better.
	* Starting in lane 1, it immediately changed to lane 0, taking a 0.05 penalty for that (trivial), then
	  stayed there decisively. Accel performance was similar to lane 0.
	* Starting in lane 2, it tried to make an illegal lane change in first time step, so crashed.  This is
	  repeatable 5 times, but with slightly varying action outputs.
* Mods for next run:
	* reset() printing lane selection to verify that it is training in lane 2. Verified it is choosing all 3
	  lanes randomly, so turned this off again to avoid clutter.
	* Tuning with choice of random seeds, based on a recent article I read.
	* Increased max LR quite a bit, since rewards tend to be small (O(1)), and therefore gradient is not large.
	* Increased noise slightly, from 0.5 to 0.6 magnitude of sigma.

12/7/22:
* Results of prev run:
	* Trial 0 had LR = 7.99e-6; it drives lane 2 all the way, but runs off the end rather than change lanes.
	* Trial 7 had LR = 1.62e-5; make immediate lane chg in lane 2, so goes off road.
	* Trial 10 had LR = 1.9e-3
	* I verified that both the raw & scaled obs vectors are correct.
	* Accel is stubbornly small, positive throughout all runs, regardless of speed & reward.
* Ideas to try (from reviewing above history):
	* Reduce size of NN
	* Play with LR schedule
	* Turn off random start distance during training (force it to complete full route every time)
	* Train in only lane 2 to see if it can at least learn that one
	* Turn off jerk penalty
	* Play with noise magnitude
* Mods for next run:
	* Forcing lane ID to be 2 always (in reset())
	* Commented out jerk penalty
	* Results:
		* 4 trials had mean reward peaks very close to 0 (> -10), but dropped off fast afterward.  These
		  had LR of 1.4e-4, 1.4e-4 and 5.0e-5.
		* All had mean reward that stayed at -50 for a long time, then several started climbing fast
		  around 500k to 600k steps after max reward suddenly jumped from -50 to +10.
		* No min reward ever exceeded -50.
		* Running trial 3 inference made it to the end!  It had several lane changes, which is fine.
		* Accelerations were somewhat sporadic, but much more aggressive at times.
		* Found a defect in geometry code that repositioned the vehicle backwards one step after making
		  the lane change.
* Mods for next run:
	* Fixed distance calc during lane change. I can't believe this had a significant impact on training,
	  although it did make the distance signal discontinuous, which could have had some effect.
	* Added tuning options for NN size.
	* Trial 3 performed well, with a mean reward hovering close to +10 for many iterations before finally
	  tanking.  Inference runs on it, in lane 2, performed ideally, holding the speed limit and changing lane
	  toward the end of lane 2.  When started in lane 1, it immediately tried to change lanes left too much.
	  This one had a NN structure of [64, 24] and LR = 7.8e-5.
	* None of the others achieved a promising mean reward, so I don't feel I'm really finding the sweet spot.

12/11/22:
* Mods for next run, which is still only training on lane 2 start:
	* Fixed the NN structure at [64, 24] and tuned for noise magnitude (still decaying to 0.04 at 1M steps).
	* Two very promising trials were terminated prematurely due to stop criteria taking slopes over way too
	  many iterations.  I need to reduce this, and also print out more details when it decides to stop.
	* Three of the first 11 trials peaked between 0 and +10 (mean reward); not great performance, but decent.
	  Inference on two of these shows good performance.  Their LRs were 7.5e-5, 1.1e-4, 9.5e-4, and they
	  used noise magnituce (stddev) of 0.61, 0.25, 0.17, respectively. So I believe I have this lane change
	  nailed.
* Mods for next run:
	* Reduced the avg_over_latest param from 300 to 60 iterations, because I believe this is iters, not
	  steps. I added better output on stop condition to help confirm this.
	* Zeroing in tuning to LRs in the range of success above, as well as noise magnitudes in the lower end
	  of that range.  
	* Reinstate all 3 lanes as candidate initial conditions (mod to reset()).

12/12/22:
	* Trials 6 & 8 peaked at mean reward ~5. Inference run shows it is pretty good, but willing to accept a lot
	  of speed penalty, with only very tiny adjustments to accel.  They both performed extraneous lane change
	  to lane 0 if starting in 1 or 2, which cost an extra ~0.2 penalty point.
	* Jerk penalty has been turned off for some time, so it is learning smooth acceleration from other means.
* Next run:
	* Increased size of NN from [64, 24] to [64, 40]
	* Turned off randomized start distance during training (forcing all starts to be at beginning of lane).
	* The first 6 trials here were lousy (one peaked at 0 and one peaked at -5), indicating that maybe
	  randomized start distance is still needed, so I aborted the run.

* Next run:
	* Increased num workers from 8 to 12, keeping the rollout fragment length = 200, so train batch size
	  increased to 2400.  Hoping things will move faster now.
	* Turned the randomized start distance back on.

12/13/22:
	* It doesn't appear that the randomized start distance had any particular effect, which was expected,
	  since it had already learned to travel the full length of the course.
	* A couple of the trials peaked above 0, and one got to +10 for mean reward. However, inference on it
	  showed pretty loose speed control.  It had no problem reaching way into the penalty areas for extended
	  periods.  Although it did seem to sense it didn't want to be there, the jerk went negative as soon as
	  it entered the high speed area, but it was so small that the accel took a long time to reverse the
	  incursion. With jerk penalty off, I don't understand why it won't change accels more quickly.

* Next run:
	* Based on successful straight-lane results on 11/19, with the more aggressive jerk performance, I am
	  going back to that [64, 48, 8] NN structure.

12/14/22:
	* This run was worse than the previous. Only one trial reached above 0 mean reward, but stayed < 5.
	  It had LR = 8.8e-5 and noise stddev = 0.28.
	* I feel like LR annealing is going to be important to get anything better.
